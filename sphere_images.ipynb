{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rotations in icosahedral group: 60\n",
      "Length of orbit: 180\n",
      "Unique points in orbit: 166\n",
      "[0.25308174 0.9455306  0.20474743]\n",
      "[[ 0.30901699 -0.80901699  0.5       ]\n",
      " [-0.80901699 -0.5        -0.30901699]\n",
      " [ 0.5        -0.30901699 -0.80901699]]\n"
     ]
    }
   ],
   "source": [
    "phi = (np.sqrt(5)+1)/2 #golden angle helps define verticies of icosahedron\n",
    "icosahedral_group = R.create_group('I') #rotation group only, 60 elements\n",
    "print(f'Number of rotations in icosahedral group: {len(icosahedral_group)}')\n",
    "rotation_matrices = icosahedral_group.as_matrix()\n",
    "d=1/3\n",
    "initial_point = (1-d)*np.array([1,phi,0])+d*np.array([0,1+2*phi,phi])\n",
    "#initial_point = np.array([1,1+3*phi,phi]) #This point does not lie on any axes of symmetry\n",
    "second_point = np.array([1,1+phi,phi])\n",
    "second_point = second_point/np.linalg.norm(second_point)\n",
    "third_point = np.array([1,phi,0])\n",
    "third_point = third_point/np.linalg.norm(third_point)\n",
    "initial_point = initial_point/np.linalg.norm(initial_point)\n",
    "orbit = []\n",
    "\n",
    "for M in rotation_matrices: #apply every rotation to point\n",
    "    new_point = M@initial_point\n",
    "    orbit.append(new_point)\n",
    "\n",
    "for M in rotation_matrices: #apply every rotation to second point\n",
    "    new_point = M@second_point\n",
    "    orbit.append(new_point)\n",
    "\n",
    "for M in rotation_matrices: #apply every rotation to second point\n",
    "    new_point = M@third_point\n",
    "    orbit.append(new_point)\n",
    "\n",
    "print(f'Length of orbit: {len(orbit)}')\n",
    "unique_orbit = np.unique(np.array(orbit), axis=0) #check uniqueness, in case of accidentally generating a subgroup\n",
    "print(f'Unique points in orbit: {np.array(unique_orbit).shape[0]}')\n",
    "x, y, z = np.array(orbit).T\n",
    "print(initial_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band: (72, 3), half-sphere: (78, 3), checkerboard: (92, 3), polar caps: (92, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def half_sphere(p):\n",
    "    # Ensure p is a numpy array for element-wise operations\n",
    "    p = np.array(p)\n",
    "    \n",
    "    # Handle the case when p[0] is 0 to avoid division by zero\n",
    "    theta = np.arctan2(p[1], p[0])  # arctan2 handles division by zero and all quadrants\n",
    "    phi = np.arccos(p[2] / np.linalg.norm(p))  # Normalize p[2] to the length of p for safety\n",
    "\n",
    "    return 1 if 0 < theta < np.pi else 0\n",
    "\n",
    "def great_band(p):\n",
    "    p = np.array(p) / np.linalg.norm(p)\n",
    "    \n",
    "    \n",
    "    theta = np.arctan2(p[1], p[0])  # Azimuthal angle\n",
    "    phi = np.arccos(p[2])  # Polar angle\n",
    "    \n",
    "    \n",
    "    band_half_width = np.pi / 6  \n",
    "    \n",
    "    # Return 1 if the point is within the band, else return 0\n",
    "    if (phi > (np.pi / 2 - band_half_width)) and (phi < (np.pi / 2 + band_half_width)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def checkerboard(p):\n",
    "    p = np.array(p) / np.linalg.norm(p)\n",
    "    \n",
    "    theta = np.arctan2(p[1], p[0])  # Azimuthal angle\n",
    "    phi = np.arccos(p[2])  # Polar angle\n",
    "    \n",
    "    theta = theta % (2 * np.pi)\n",
    "    \n",
    "    # Define the number of divisions in each direction\n",
    "    # More divisions will create a finer checkerboard pattern\n",
    "    divisions_theta = 4  # Divisions in azimuthal direction\n",
    "    divisions_phi = 2  # Divisions in polar direction\n",
    "    \n",
    "    # Calculate the width of each division\n",
    "    width_theta = (2 * np.pi) / divisions_theta\n",
    "    width_phi = np.pi / divisions_phi\n",
    "    \n",
    "    # Determine the index of the division each angle falls into\n",
    "    index_theta = int(theta // width_theta)\n",
    "    index_phi = int(phi // width_phi)\n",
    "    \n",
    "    # Return 1 if the sum of the indices is even (checkerboard pattern), else return 0\n",
    "    return 1 if (index_theta + index_phi) % 2 == 0 else 0\n",
    "\n",
    "def polar_caps(p, cap_width=np.pi/3):\n",
    "    p = np.array(p) / np.linalg.norm(p)\n",
    "    phi = np.arccos(p[2])  # Polar angle\n",
    "    if phi < cap_width or phi > (np.pi - cap_width):\n",
    "        return 1 \n",
    "    return 0  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "half_sphere_output = np.array([half_sphere(point) for point in orbit])\n",
    "band_output = np.array([great_band(point) for point in orbit])\n",
    "checkerboard_output = np.array([checkerboard(point) for point in orbit])\n",
    "polar_caps_output = np.array([polar_caps(point) for point in orbit])\n",
    "\n",
    "filtered_orbit_band = np.array(orbit)[band_output == 1]\n",
    "filtered_orbit_checkerboard = np.array(orbit)[checkerboard_output == 1]\n",
    "filtered_orbit_polar_caps = np.array(orbit)[polar_caps_output == 1]\n",
    "filtered_orbit_half_sphere = np.array(orbit)[half_sphere_output == 1]\n",
    "print(f'band: {filtered_orbit_band.shape}, half-sphere: {filtered_orbit_half_sphere.shape}, checkerboard: {filtered_orbit_checkerboard.shape}, polar caps: {filtered_orbit_polar_caps.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "size": 2
         },
         "mode": "markers",
         "scene": "scene",
         "type": "scatter3d",
         "x": [
          -0.20474743242581603,
          0.20474743242581603,
          0.20474743242581603,
          -0.20474743242581603,
          0.33128830477424265,
          -1.521873481762698e-17,
          -0.7407831696258748,
          -0.20474743242581597,
          0.7407831696258748,
          0.6142422972774481,
          -0.3312883047742426,
          0.3312883047742426,
          0.20474743242581597,
          -0.33128830477424265,
          1.521873481762698e-17,
          -0.6142422972774481,
          -0.7407831696258748,
          -0.7407831696258748,
          0.7407831696258748,
          0.7407831696258748,
          0.6142422972774481,
          -0.3312883047742426,
          -0.20474743242581597,
          1.521873481762698e-17,
          -0.33128830477424265,
          -0.6142422972774481,
          0.20474743242581597,
          0.3312883047742426,
          -0.7407831696258748,
          0.7407831696258748,
          -1.521873481762698e-17,
          0.33128830477424265,
          -0.5,
          0.5,
          0.5,
          -0.5,
          0.5,
          -0.5,
          0,
          0.4999999999999999,
          0,
          -0.5,
          -0.4999999999999999,
          -0.5,
          0.5,
          0.4999999999999999,
          0,
          -0.5,
          -0.4999999999999999,
          0,
          0.5,
          0.5,
          0,
          -0.85065080835204,
          0.85065080835204,
          0,
          -0.85065080835204,
          0,
          0,
          0.85065080835204,
          2.2854319905277973e-19,
          2.2854319905277973e-19,
          -0.85065080835204,
          -0.85065080835204,
          4.722060566742181e-17,
          0.85065080835204,
          0.85065080835204,
          4.722060566742181e-17,
          -4.722060566742181e-17,
          -4.722060566742181e-17,
          -2.2854319905277973e-19,
          -2.2854319905277973e-19,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          4.722060566742181e-17,
          4.722060566742181e-17,
          -2.2854319905277973e-19,
          -2.2854319905277973e-19,
          -0.85065080835204,
          -4.722060566742181e-17,
          -4.722060566742181e-17,
          -0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          2.2854319905277973e-19,
          2.2854319905277973e-19
         ],
         "y": [
          -0.2530817446968533,
          0.2530817446968533,
          -0.2530817446968533,
          0.2530817446968533,
          -0.584370049471096,
          -0.7891174818969121,
          0.33128830477424265,
          0.2530817446968533,
          0.33128830477424276,
          6.804799202925976e-17,
          0.5843700494710959,
          0.5843700494710959,
          0.2530817446968533,
          0.584370049471096,
          0.7891174818969121,
          6.804799202925976e-17,
          0.33128830477424276,
          -0.33128830477424265,
          -0.33128830477424265,
          -0.33128830477424276,
          -6.804799202925976e-17,
          -0.5843700494710959,
          -0.2530817446968533,
          -0.7891174818969121,
          -0.584370049471096,
          -6.804799202925976e-17,
          -0.2530817446968533,
          -0.5843700494710959,
          -0.33128830477424276,
          0.33128830477424265,
          0.7891174818969121,
          0.584370049471096,
          -0.3090169943749474,
          0.3090169943749474,
          -0.3090169943749474,
          0.3090169943749474,
          -0.30901699437494745,
          0.30901699437494745,
          0,
          -0.30901699437494734,
          0,
          0.30901699437494745,
          -0.30901699437494734,
          -0.30901699437494745,
          -0.30901699437494745,
          0.30901699437494734,
          0,
          -0.30901699437494745,
          0.30901699437494734,
          0,
          0.30901699437494745,
          0.30901699437494745,
          -0.5257311121191336,
          0,
          0,
          0.5257311121191336,
          0,
          -0.5257311121191336,
          0.5257311121191336,
          0,
          -0.5257311121191337,
          -0.5257311121191337,
          0,
          0,
          0.5257311121191336,
          1.0296030009773242e-16,
          1.0296030009773242e-16,
          0.5257311121191336,
          0.5257311121191336,
          0.5257311121191336,
          0.5257311121191337,
          0.5257311121191337,
          1.0296030009773242e-16,
          1.0296030009773242e-16,
          0,
          0,
          0,
          0,
          -1.0296030009773242e-16,
          -1.0296030009773242e-16,
          -0.5257311121191336,
          -0.5257311121191336,
          -0.5257311121191337,
          -0.5257311121191337,
          -1.0296030009773242e-16,
          -0.5257311121191336,
          -0.5257311121191336,
          -1.0296030009773242e-16,
          0,
          0,
          0.5257311121191337,
          0.5257311121191337
         ],
         "z": [
          0.9455306020516908,
          0.9455306020516908,
          -0.9455306020516908,
          -0.9455306020516908,
          0.7407831696258748,
          -0.6142422972774481,
          -0.5843700494710959,
          0.9455306020516908,
          -0.584370049471096,
          0.7891174818969121,
          -0.7407831696258748,
          0.7407831696258748,
          -0.9455306020516908,
          0.7407831696258748,
          -0.6142422972774481,
          -0.7891174818969121,
          0.584370049471096,
          0.5843700494710959,
          -0.5843700494710959,
          0.584370049471096,
          -0.7891174818969121,
          0.7407831696258748,
          -0.9455306020516908,
          0.6142422972774481,
          -0.7407831696258748,
          0.7891174818969121,
          0.9455306020516908,
          -0.7407831696258748,
          -0.584370049471096,
          0.5843700494710959,
          0.6142422972774481,
          -0.7407831696258748,
          0.8090169943749475,
          0.8090169943749475,
          -0.8090169943749475,
          -0.8090169943749475,
          0.8090169943749473,
          -0.8090169943749475,
          1,
          0.8090169943749475,
          -1,
          0.8090169943749473,
          -0.8090169943749475,
          0.8090169943749475,
          -0.8090169943749475,
          -0.8090169943749475,
          -1,
          -0.8090169943749473,
          0.8090169943749475,
          1,
          0.8090169943749475,
          -0.8090169943749473,
          0.85065080835204,
          0.5257311121191336,
          -0.5257311121191336,
          0.85065080835204,
          -0.5257311121191336,
          -0.85065080835204,
          -0.85065080835204,
          0.5257311121191336,
          0.85065080835204,
          -0.85065080835204,
          0.5257311121191336,
          -0.5257311121191336,
          0.85065080835204,
          -0.5257311121191337,
          0.5257311121191337,
          -0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          -0.5257311121191337,
          0.5257311121191337,
          0.5257311121191336,
          -0.5257311121191336,
          0.5257311121191336,
          -0.5257311121191336,
          0.5257311121191337,
          -0.5257311121191337,
          0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          0.5257311121191337,
          0.85065080835204,
          -0.85065080835204,
          -0.5257311121191337,
          0.5257311121191336,
          -0.5257311121191336,
          0.85065080835204,
          -0.85065080835204
         ]
        },
        {
         "marker": {
          "color": "blue",
          "size": 2
         },
         "mode": "markers",
         "scene": "scene2",
         "type": "scatter3d",
         "x": [
          0.2530817446968533,
          -0.2530817446968533,
          -0.2530817446968533,
          0.2530817446968533,
          -0.20474743242581603,
          -0.9455306020516908,
          0.9455306020516908,
          0.20474743242581603,
          -0.9455306020516908,
          0.20474743242581603,
          -0.20474743242581603,
          0.9455306020516908,
          -1.521873481762698e-17,
          -0.5843700494710959,
          -0.7407831696258748,
          0.6142422972774481,
          -0.3312883047742426,
          0.5843700494710959,
          0.3312883047742426,
          1.521873481762698e-17,
          -0.7407831696258748,
          0.7891174818969121,
          0.7407831696258748,
          -0.3312883047742426,
          0.5843700494710959,
          -0.6142422972774481,
          0.3312883047742426,
          0.7407831696258748,
          -0.7891174818969121,
          -0.5843700494710959,
          0.3090169943749474,
          -0.3090169943749474,
          -0.3090169943749474,
          0.3090169943749474,
          -0.5,
          -0.8090169943749475,
          0.8090169943749475,
          0.5,
          -0.8090169943749475,
          0.5,
          -0.5,
          0.8090169943749475,
          5.551115123125783e-17,
          -0.8090169943749475,
          -0.5,
          0,
          -0.3090169943749474,
          0.8090169943749475,
          -5.551115123125783e-17,
          0.3090169943749474,
          0,
          -0.5,
          0.5,
          -0.3090169943749474,
          0,
          0.8090169943749475,
          -5.551115123125783e-17,
          0,
          0.3090169943749474,
          0.5,
          5.551115123125783e-17,
          -0.8090169943749475,
          0.5257311121191336,
          -0.5257311121191336,
          -0.85065080835204,
          0,
          0,
          0.85065080835204,
          2.2854319905277973e-19,
          -0.85065080835204,
          0.85065080835204,
          4.722060566742181e-17,
          -4.722060566742181e-17,
          -2.2854319905277973e-19,
          -0.85065080835204,
          -0.85065080835204,
          0.5257311121191337,
          0.5257311121191337,
          0.85065080835204,
          0.85065080835204,
          4.722060566742181e-17,
          0.5257311121191336,
          0.5257311121191336,
          -2.2854319905277973e-19,
          -0.85065080835204,
          -4.722060566742181e-17,
          -0.5257311121191337,
          0.85065080835204,
          -0.5257311121191337,
          -0.5257311121191336,
          2.2854319905277973e-19,
          -0.5257311121191336
         ],
         "y": [
          -0.9455306020516908,
          0.9455306020516908,
          -0.9455306020516908,
          0.9455306020516908,
          -0.2530817446968533,
          -0.20474743242581603,
          -0.20474743242581603,
          0.2530817446968533,
          0.20474743242581603,
          -0.2530817446968533,
          0.2530817446968533,
          0.20474743242581603,
          -0.7891174818969121,
          -0.7407831696258748,
          0.33128830477424265,
          6.804799202925976e-17,
          0.5843700494710959,
          0.7407831696258748,
          0.5843700494710959,
          0.7891174818969121,
          -0.33128830477424265,
          -0.6142422972774482,
          -0.33128830477424265,
          -0.5843700494710959,
          -0.7407831696258748,
          -6.804799202925976e-17,
          -0.5843700494710959,
          0.33128830477424265,
          0.6142422972774482,
          0.7407831696258748,
          -0.8090169943749475,
          0.8090169943749475,
          -0.8090169943749475,
          0.8090169943749475,
          -0.3090169943749474,
          -0.5,
          -0.5,
          0.3090169943749474,
          0.5,
          -0.3090169943749474,
          0.3090169943749474,
          0.5,
          -1,
          -0.5,
          0.30901699437494745,
          0,
          0.8090169943749475,
          0.5,
          1,
          0.8090169943749475,
          0,
          -0.30901699437494745,
          -0.30901699437494745,
          -0.8090169943749475,
          0,
          -0.5,
          -1,
          0,
          -0.8090169943749475,
          0.30901699437494745,
          1,
          0.5,
          -0.85065080835204,
          0.85065080835204,
          0,
          -0.5257311121191336,
          0.5257311121191336,
          0,
          -0.5257311121191337,
          0,
          1.0296030009773242e-16,
          0.5257311121191336,
          0.5257311121191336,
          0.5257311121191337,
          1.0296030009773242e-16,
          0,
          -0.85065080835204,
          -0.85065080835204,
          0,
          -1.0296030009773242e-16,
          -0.5257311121191336,
          -0.85065080835204,
          -0.85065080835204,
          -0.5257311121191337,
          -1.0296030009773242e-16,
          -0.5257311121191336,
          0.85065080835204,
          0,
          0.85065080835204,
          0.85065080835204,
          0.5257311121191337,
          0.85065080835204
         ],
         "z": [
          -0.20474743242581603,
          -0.20474743242581603,
          0.20474743242581603,
          0.20474743242581603,
          0.9455306020516908,
          0.2530817446968533,
          -0.2530817446968533,
          0.9455306020516908,
          -0.2530817446968533,
          -0.9455306020516908,
          -0.9455306020516908,
          0.2530817446968533,
          -0.6142422972774481,
          0.33128830477424265,
          -0.5843700494710959,
          0.7891174818969121,
          -0.7407831696258748,
          0.33128830477424265,
          0.7407831696258748,
          -0.6142422972774481,
          0.5843700494710959,
          -1.2536840798001934e-17,
          -0.5843700494710959,
          0.7407831696258748,
          -0.33128830477424265,
          0.7891174818969121,
          -0.7407831696258748,
          0.5843700494710959,
          -1.2536840798001934e-17,
          -0.33128830477424265,
          -0.5,
          -0.5,
          0.5,
          0.5,
          0.8090169943749475,
          0.3090169943749474,
          -0.3090169943749474,
          0.8090169943749475,
          -0.3090169943749474,
          -0.8090169943749475,
          -0.8090169943749475,
          0.3090169943749474,
          2.7755575615628914e-17,
          0.30901699437494745,
          -0.8090169943749475,
          1,
          -0.5,
          0.30901699437494745,
          2.7755575615628914e-17,
          0.5,
          -1,
          0.8090169943749475,
          -0.8090169943749475,
          0.5,
          -1,
          -0.30901699437494745,
          -2.7755575615628914e-17,
          1,
          -0.5,
          0.8090169943749475,
          -2.7755575615628914e-17,
          -0.30901699437494745,
          0,
          0,
          0.5257311121191336,
          -0.85065080835204,
          -0.85065080835204,
          0.5257311121191336,
          -0.85065080835204,
          0.5257311121191336,
          0.5257311121191337,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          0.5257311121191337,
          0.5257311121191336,
          -5.573969443031061e-17,
          5.573969443031061e-17,
          0.5257311121191336,
          0.5257311121191337,
          -0.85065080835204,
          4.722060566742181e-17,
          -4.722060566742181e-17,
          -0.85065080835204,
          0.5257311121191337,
          -0.85065080835204,
          5.573969443031061e-17,
          0.5257311121191336,
          -5.573969443031061e-17,
          -4.722060566742181e-17,
          -0.85065080835204,
          4.722060566742181e-17
         ]
        },
        {
         "marker": {
          "color": "blue",
          "size": 2
         },
         "mode": "markers",
         "scene": "scene3",
         "type": "scatter3d",
         "x": [
          -0.2530817446968533,
          0.2530817446968533,
          0.20474743242581603,
          -0.9455306020516908,
          -0.20474743242581603,
          0.9455306020516908,
          0.7891174818969121,
          0.584370049471096,
          -0.9455306020516908,
          -0.7407831696258748,
          -0.20474743242581597,
          0.7407831696258748,
          0.6142422972774481,
          -0.3312883047742426,
          0.5843700494710959,
          0.2530817446968533,
          0.3312883047742426,
          0.20474743242581597,
          -0.33128830477424265,
          1.521873481762698e-17,
          -0.7407831696258748,
          -0.584370049471096,
          0.7407831696258748,
          0.9455306020516908,
          -0.7891174818969121,
          -0.2530817446968533,
          -1.521873481762698e-17,
          0.33128830477424265,
          -0.5843700494710959,
          -0.3090169943749474,
          0.3090169943749474,
          0.5,
          -0.8090169943749475,
          -0.5,
          0.8090169943749475,
          0.8090169943749475,
          0.30901699437494745,
          -0.5,
          0.8090169943749473,
          -0.3090169943749474,
          0.8090169943749475,
          -5.551115123125783e-17,
          0.3090169943749474,
          -0.5,
          0.30901699437494745,
          -0.8090169943749473,
          0.4999999999999999,
          -0.4999999999999999,
          -0.30901699437494745,
          0.5,
          1,
          -0.8090169943749475,
          5.551115123125783e-17,
          -0.30901699437494745,
          0.5,
          -0.8090169943749475,
          -0.5257311121191336,
          0.5257311121191336,
          0,
          0,
          0.5257311121191337,
          0.5257311121191337,
          4.722060566742181e-17,
          0.85065080835204,
          0.85065080835204,
          4.722060566742181e-17,
          0.5257311121191336,
          0.5257311121191336,
          -4.722060566742181e-17,
          -4.722060566742181e-17,
          -2.2854319905277973e-19,
          -2.2854319905277973e-19,
          -0.5257311121191337,
          -0.5257311121191337,
          -0.5257311121191336,
          2.2854319905277973e-19,
          2.2854319905277973e-19,
          -0.5257311121191336
         ],
         "y": [
          0.9455306020516908,
          0.9455306020516908,
          0.2530817446968533,
          0.20474743242581603,
          0.2530817446968533,
          0.20474743242581603,
          0.6142422972774482,
          0.7407831696258749,
          0.20474743242581603,
          0.33128830477424265,
          0.2530817446968533,
          0.33128830477424276,
          6.804799202925976e-17,
          0.5843700494710959,
          0.7407831696258748,
          0.9455306020516908,
          0.5843700494710959,
          0.2530817446968533,
          0.584370049471096,
          0.7891174818969121,
          0.33128830477424276,
          0.7407831696258749,
          0.33128830477424265,
          0.20474743242581603,
          0.6142422972774482,
          0.9455306020516908,
          0.7891174818969121,
          0.584370049471096,
          0.7407831696258748,
          0.8090169943749475,
          0.8090169943749475,
          0.3090169943749474,
          0.5,
          0.3090169943749474,
          0.5,
          0.5,
          0.8090169943749475,
          0.30901699437494745,
          0.5000000000000001,
          0.8090169943749475,
          0.5,
          1,
          0.8090169943749475,
          0.30901699437494745,
          0.8090169943749475,
          0.5000000000000001,
          0.30901699437494734,
          0.30901699437494734,
          0.8090169943749475,
          0.30901699437494745,
          5.551115123125783e-17,
          0.5,
          1,
          0.8090169943749475,
          0.30901699437494745,
          0.5,
          0.85065080835204,
          0.85065080835204,
          0.5257311121191336,
          0.5257311121191336,
          0.85065080835204,
          0.85065080835204,
          0.5257311121191336,
          1.0296030009773242e-16,
          1.0296030009773242e-16,
          0.5257311121191336,
          0.85065080835204,
          0.85065080835204,
          0.5257311121191336,
          0.5257311121191336,
          0.5257311121191337,
          0.5257311121191337,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.5257311121191337,
          0.5257311121191337,
          0.85065080835204
         ],
         "z": [
          -0.20474743242581603,
          0.20474743242581603,
          0.9455306020516908,
          -0.2530817446968533,
          -0.9455306020516908,
          0.2530817446968533,
          1.2536840798001934e-17,
          -0.3312883047742427,
          0.25308174469685324,
          -0.5843700494710959,
          0.9455306020516908,
          -0.584370049471096,
          0.7891174818969121,
          -0.7407831696258748,
          0.33128830477424265,
          -0.20474743242581597,
          0.7407831696258748,
          -0.9455306020516908,
          0.7407831696258748,
          -0.6142422972774481,
          0.584370049471096,
          0.3312883047742427,
          0.5843700494710959,
          -0.25308174469685324,
          -1.2536840798001934e-17,
          0.20474743242581597,
          0.6142422972774481,
          -0.7407831696258748,
          -0.33128830477424265,
          -0.5,
          0.5,
          0.8090169943749475,
          -0.3090169943749474,
          -0.8090169943749475,
          0.3090169943749474,
          -0.3090169943749474,
          -0.5,
          -0.8090169943749475,
          -0.30901699437494745,
          -0.5,
          0.30901699437494745,
          2.7755575615628914e-17,
          0.5,
          0.8090169943749473,
          -0.4999999999999999,
          0.30901699437494745,
          -0.8090169943749475,
          0.8090169943749475,
          0.5,
          0.8090169943749475,
          5.551115123125783e-17,
          0.3090169943749474,
          -2.7755575615628914e-17,
          0.4999999999999999,
          -0.8090169943749473,
          -0.30901699437494745,
          0,
          0,
          0.85065080835204,
          -0.85065080835204,
          5.573969443031061e-17,
          -5.573969443031061e-17,
          0.85065080835204,
          -0.5257311121191337,
          0.5257311121191337,
          -0.85065080835204,
          -4.722060566742181e-17,
          4.722060566742181e-17,
          0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          5.573969443031061e-17,
          -5.573969443031061e-17,
          -4.722060566742181e-17,
          0.85065080835204,
          -0.85065080835204,
          4.722060566742181e-17
         ]
        },
        {
         "marker": {
          "color": "blue",
          "size": 2
         },
         "mode": "markers",
         "scene": "scene4",
         "type": "scatter3d",
         "x": [
          0.2530817446968533,
          -0.2530817446968533,
          -0.2530817446968533,
          0.2530817446968533,
          -0.9455306020516908,
          0.9455306020516908,
          -0.9455306020516908,
          0.9455306020516908,
          0.7891174818969121,
          0.584370049471096,
          -0.2530817446968533,
          -0.5843700494710959,
          -0.9455306020516908,
          0.5843700494710959,
          0.2530817446968533,
          0.7891174818969121,
          0.584370049471096,
          -0.9455306020516908,
          0.9455306020516908,
          -0.584370049471096,
          -0.7891174818969121,
          0.5843700494710959,
          0.2530817446968533,
          -0.584370049471096,
          0.9455306020516908,
          -0.7891174818969121,
          -0.2530817446968533,
          -0.5843700494710959,
          -0.8090169943749475,
          0.8090169943749475,
          -0.8090169943749475,
          0.8090169943749475,
          0.8090169943749475,
          5.551115123125783e-17,
          -0.8090169943749475,
          -1,
          0.8090169943749473,
          0.8090169943749475,
          -5.551115123125783e-17,
          -0.8090169943749473,
          0.8090169943749475,
          -1,
          1,
          0.8090169943749473,
          -0.8090169943749475,
          0.8090169943749475,
          -5.551115123125783e-17,
          -0.8090169943749473,
          1,
          -0.8090169943749475,
          5.551115123125783e-17,
          -0.8090169943749475,
          0.5257311121191336,
          -0.5257311121191336,
          -0.5257311121191336,
          0.5257311121191336,
          0.5257311121191337,
          0.5257311121191337,
          -0.5257311121191336,
          -0.5257311121191336,
          0.5257311121191336,
          0.5257311121191336,
          0.5257311121191337,
          0.5257311121191337,
          -0.5257311121191337,
          -0.5257311121191337,
          0.5257311121191336,
          0.5257311121191336,
          -0.5257311121191337,
          -0.5257311121191337,
          -0.5257311121191336,
          -0.5257311121191336
         ],
         "y": [
          -0.9455306020516908,
          0.9455306020516908,
          -0.9455306020516908,
          0.9455306020516908,
          -0.20474743242581603,
          -0.20474743242581603,
          0.20474743242581603,
          0.20474743242581603,
          0.6142422972774482,
          0.7407831696258749,
          -0.9455306020516908,
          -0.7407831696258748,
          0.20474743242581603,
          0.7407831696258748,
          0.9455306020516908,
          -0.6142422972774482,
          -0.7407831696258749,
          -0.20474743242581603,
          -0.20474743242581603,
          -0.7407831696258749,
          -0.6142422972774482,
          -0.7407831696258748,
          -0.9455306020516908,
          0.7407831696258749,
          0.20474743242581603,
          0.6142422972774482,
          0.9455306020516908,
          0.7407831696258748,
          -0.5,
          -0.5,
          0.5,
          0.5,
          0.5,
          -1,
          -0.5,
          5.551115123125783e-17,
          0.5000000000000001,
          0.5,
          1,
          0.5000000000000001,
          -0.5,
          -5.551115123125783e-17,
          -5.551115123125783e-17,
          -0.5000000000000001,
          -0.5,
          -0.5,
          -1,
          -0.5000000000000001,
          5.551115123125783e-17,
          0.5,
          1,
          0.5,
          -0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          -0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204,
          0.85065080835204
         ],
         "z": [
          -0.20474743242581603,
          -0.20474743242581603,
          0.20474743242581603,
          0.20474743242581603,
          0.2530817446968533,
          -0.2530817446968533,
          -0.2530817446968533,
          0.2530817446968533,
          1.2536840798001934e-17,
          -0.3312883047742427,
          -0.20474743242581597,
          0.33128830477424265,
          0.25308174469685324,
          0.33128830477424265,
          -0.20474743242581597,
          -1.2536840798001934e-17,
          0.3312883047742427,
          -0.25308174469685324,
          0.25308174469685324,
          -0.3312883047742427,
          1.2536840798001934e-17,
          -0.33128830477424265,
          0.20474743242581597,
          0.3312883047742427,
          -0.25308174469685324,
          -1.2536840798001934e-17,
          0.20474743242581597,
          -0.33128830477424265,
          0.3090169943749474,
          -0.3090169943749474,
          -0.3090169943749474,
          0.3090169943749474,
          -0.3090169943749474,
          2.7755575615628914e-17,
          0.30901699437494745,
          -5.551115123125783e-17,
          -0.30901699437494745,
          0.30901699437494745,
          2.7755575615628914e-17,
          0.30901699437494745,
          0.3090169943749474,
          5.551115123125783e-17,
          -5.551115123125783e-17,
          0.30901699437494745,
          -0.3090169943749474,
          -0.30901699437494745,
          -2.7755575615628914e-17,
          -0.30901699437494745,
          5.551115123125783e-17,
          0.3090169943749474,
          -2.7755575615628914e-17,
          -0.30901699437494745,
          0,
          0,
          0,
          0,
          5.573969443031061e-17,
          -5.573969443031061e-17,
          4.722060566742181e-17,
          -4.722060566742181e-17,
          -4.722060566742181e-17,
          4.722060566742181e-17,
          -5.573969443031061e-17,
          5.573969443031061e-17,
          -5.573969443031061e-17,
          5.573969443031061e-17,
          4.722060566742181e-17,
          -4.722060566742181e-17,
          5.573969443031061e-17,
          -5.573969443031061e-17,
          -4.722060566742181e-17,
          4.722060566742181e-17
         ]
        }
       ],
       "layout": {
        "height": 800,
        "scene": {
         "domain": {
          "x": [
           0,
           0.45
          ],
          "y": [
           0.575,
           1
          ]
         }
        },
        "scene2": {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0.575,
           1
          ]
         }
        },
        "scene3": {
         "domain": {
          "x": [
           0,
           0.45
          ],
          "y": [
           0,
           0.425
          ]
         }
        },
        "scene4": {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0,
           0.425
          ]
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Different Spherical Shapes"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a 2x2 subplot\n",
    "fig = make_subplots(rows=2, cols=2, specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}],\n",
    "                                           [{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "\n",
    "# Subplot 1: Polar Caps\n",
    "fig.add_trace(go.Scatter3d(x=x[polar_caps_output == 1], y=y[polar_caps_output == 1], z=z[polar_caps_output == 1], mode='markers', marker=dict(size=2, color='blue')), row=1, col=1)\n",
    "\n",
    "# Subplot 2: Checkerboard\n",
    "fig.add_trace(go.Scatter3d(x=x[checkerboard_output == 1], y=y[checkerboard_output == 1], z=z[checkerboard_output == 1], mode='markers', marker=dict(size=2, color='blue')), row=1, col=2)\n",
    "\n",
    "# Subplot 3: Half-Sphere\n",
    "fig.add_trace(go.Scatter3d(x=x[half_sphere_output == 1], y=y[half_sphere_output == 1], z=z[half_sphere_output == 1], mode='markers', marker=dict(size=2, color='blue')), row=2, col=1)\n",
    "\n",
    "# Subplot 4: Band\n",
    "fig.add_trace(go.Scatter3d(x=x[band_output == 1], y=y[band_output == 1], z=z[band_output == 1], mode='markers', marker=dict(size=2, color='blue')), row=2, col=2)\n",
    "\n",
    "# Update layout for a better view\n",
    "fig.update_layout(height=800, width=800, title_text=\"Different Spherical Shapes\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        sequences: A list of binary sequences, each representing a shape.\n",
    "        labels: A list of integers, labels for each sequence.\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(180, 128)  # Input layer\n",
    "        self.fc2 = nn.Linear(128, 64)   # Hidden layer\n",
    "        self.fc3 = nn.Linear(64, 4)     # Output layer for 4 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3818868398666382\n",
      "Epoch 2, Loss: 1.3320045471191406\n",
      "Epoch 3, Loss: 1.290358543395996\n",
      "Epoch 4, Loss: 1.248523473739624\n",
      "Epoch 5, Loss: 1.2059574127197266\n",
      "Epoch 6, Loss: 1.163787841796875\n",
      "Epoch 7, Loss: 1.1181671619415283\n",
      "Epoch 8, Loss: 1.0702389478683472\n",
      "Epoch 9, Loss: 1.0202968120574951\n",
      "Epoch 10, Loss: 0.9700470566749573\n",
      "Epoch 11, Loss: 0.9177461862564087\n",
      "Epoch 12, Loss: 0.8644505739212036\n",
      "Epoch 13, Loss: 0.8089706897735596\n",
      "Epoch 14, Loss: 0.7526702284812927\n",
      "Epoch 15, Loss: 0.696197509765625\n",
      "Epoch 16, Loss: 0.6397899389266968\n",
      "Epoch 17, Loss: 0.5837811827659607\n",
      "Epoch 18, Loss: 0.527787446975708\n",
      "Epoch 19, Loss: 0.4721311330795288\n",
      "Epoch 20, Loss: 0.41777676343917847\n",
      "Epoch 21, Loss: 0.3654927611351013\n",
      "Epoch 22, Loss: 0.3163163959980011\n",
      "Epoch 23, Loss: 0.2706420123577118\n",
      "Epoch 24, Loss: 0.22909128665924072\n",
      "Epoch 25, Loss: 0.1919882446527481\n",
      "Epoch 26, Loss: 0.15949563682079315\n",
      "Epoch 27, Loss: 0.13158336281776428\n",
      "Epoch 28, Loss: 0.10774515569210052\n",
      "Epoch 29, Loss: 0.08788274973630905\n",
      "Epoch 30, Loss: 0.07178693264722824\n",
      "Epoch 31, Loss: 0.058662593364715576\n",
      "Epoch 32, Loss: 0.04798359423875809\n",
      "Epoch 33, Loss: 0.03936299309134483\n",
      "Epoch 34, Loss: 0.032458726316690445\n",
      "Epoch 35, Loss: 0.026850270107388496\n",
      "Epoch 36, Loss: 0.02234063297510147\n",
      "Epoch 37, Loss: 0.0186777263879776\n",
      "Epoch 38, Loss: 0.015697412192821503\n",
      "Epoch 39, Loss: 0.013265145942568779\n",
      "Epoch 40, Loss: 0.011287303641438484\n",
      "Epoch 41, Loss: 0.009664377197623253\n",
      "Epoch 42, Loss: 0.008334217593073845\n",
      "Epoch 43, Loss: 0.007233814802020788\n",
      "Epoch 44, Loss: 0.0063221086747944355\n",
      "Epoch 45, Loss: 0.005572393070906401\n",
      "Epoch 46, Loss: 0.0049533299170434475\n",
      "Epoch 47, Loss: 0.0044392552226781845\n",
      "Epoch 48, Loss: 0.00400292593985796\n",
      "Epoch 49, Loss: 0.0036298995837569237\n",
      "Epoch 50, Loss: 0.003309826832264662\n",
      "Epoch 51, Loss: 0.0030338189098984003\n",
      "Epoch 52, Loss: 0.00279493210837245\n",
      "Epoch 53, Loss: 0.002588469535112381\n",
      "Epoch 54, Loss: 0.0024094870314002037\n",
      "Epoch 55, Loss: 0.0022526481188833714\n",
      "Epoch 56, Loss: 0.002114188624545932\n",
      "Epoch 57, Loss: 0.0019915588200092316\n",
      "Epoch 58, Loss: 0.001882415497675538\n",
      "Epoch 59, Loss: 0.0017850680742412806\n",
      "Epoch 60, Loss: 0.0016977352788671851\n",
      "Epoch 61, Loss: 0.0016192002221941948\n",
      "Epoch 62, Loss: 0.0015483044553548098\n",
      "Epoch 63, Loss: 0.0014868342550471425\n",
      "Epoch 64, Loss: 0.001431579003110528\n",
      "Epoch 65, Loss: 0.0013811409007757902\n",
      "Epoch 66, Loss: 0.001335045089945197\n",
      "Epoch 67, Loss: 0.0012928454671055079\n",
      "Epoch 68, Loss: 0.0012540367897599936\n",
      "Epoch 69, Loss: 0.0012181432684883475\n",
      "Epoch 70, Loss: 0.0011849567526951432\n",
      "Epoch 71, Loss: 0.0011542396387085319\n",
      "Epoch 72, Loss: 0.0011258728336542845\n",
      "Epoch 73, Loss: 0.0010993509786203504\n",
      "Epoch 74, Loss: 0.0010749415960162878\n",
      "Epoch 75, Loss: 0.0010525262914597988\n",
      "Epoch 76, Loss: 0.0010314499959349632\n",
      "Epoch 77, Loss: 0.0010114747565239668\n",
      "Epoch 78, Loss: 0.0009924814803525805\n",
      "Epoch 79, Loss: 0.0009745299466885626\n",
      "Epoch 80, Loss: 0.0009574711439199746\n",
      "Epoch 81, Loss: 0.0009411860955879092\n",
      "Epoch 82, Loss: 0.0009255557088181376\n",
      "Epoch 83, Loss: 0.0009105802164413035\n",
      "Epoch 84, Loss: 0.0008964083390310407\n",
      "Epoch 85, Loss: 0.0008829805301502347\n",
      "Epoch 86, Loss: 0.0008701181504875422\n",
      "Epoch 87, Loss: 0.0008581191068515182\n",
      "Epoch 88, Loss: 0.000847221352159977\n",
      "Epoch 89, Loss: 0.0008365022367797792\n",
      "Epoch 90, Loss: 0.0008259020978584886\n",
      "Epoch 91, Loss: 0.0008154210518114269\n",
      "Epoch 92, Loss: 0.0008050590986385942\n",
      "Epoch 93, Loss: 0.0007948459242470562\n",
      "Epoch 94, Loss: 0.0007851686095818877\n",
      "Epoch 95, Loss: 0.0007759376894682646\n",
      "Epoch 96, Loss: 0.0007669448386877775\n",
      "Epoch 97, Loss: 0.0007580710807815194\n",
      "Epoch 98, Loss: 0.000749584287405014\n",
      "Epoch 99, Loss: 0.0007414249703288078\n",
      "Epoch 100, Loss: 0.0007333846879191697\n",
      "Epoch 101, Loss: 0.0007254040101543069\n",
      "Epoch 102, Loss: 0.0007174232159741223\n",
      "Epoch 103, Loss: 0.0007095913169905543\n",
      "Epoch 104, Loss: 0.0007020273478701711\n",
      "Epoch 105, Loss: 0.0006947014480829239\n",
      "Epoch 106, Loss: 0.0006877030828036368\n",
      "Epoch 107, Loss: 0.0006808833568356931\n",
      "Epoch 108, Loss: 0.0006739743403159082\n",
      "Epoch 109, Loss: 0.000667005660943687\n",
      "Epoch 110, Loss: 0.0006604539230465889\n",
      "Epoch 111, Loss: 0.0006540212198160589\n",
      "Epoch 112, Loss: 0.0006477076094597578\n",
      "Epoch 113, Loss: 0.0006413045921362936\n",
      "Epoch 114, Loss: 0.0006349908071570098\n",
      "Epoch 115, Loss: 0.0006288854638114572\n",
      "Epoch 116, Loss: 0.0006228099227882922\n",
      "Epoch 117, Loss: 0.0006167938117869198\n",
      "Epoch 118, Loss: 0.0006108075031079352\n",
      "Epoch 119, Loss: 0.0006049998919479549\n",
      "Epoch 120, Loss: 0.0005992219084873796\n",
      "Epoch 121, Loss: 0.0005938609247095883\n",
      "Epoch 122, Loss: 0.0005884105339646339\n",
      "Epoch 123, Loss: 0.0005829005967825651\n",
      "Epoch 124, Loss: 0.0005772715667262673\n",
      "Epoch 125, Loss: 0.0005719700129702687\n",
      "Epoch 126, Loss: 0.0005668174126185477\n",
      "Epoch 127, Loss: 0.0005616944981738925\n",
      "Epoch 128, Loss: 0.0005566311301663518\n",
      "Epoch 129, Loss: 0.0005515975644811988\n",
      "Epoch 130, Loss: 0.0005465341382659972\n",
      "Epoch 131, Loss: 0.0005417089560069144\n",
      "Epoch 132, Loss: 0.0005368242273107171\n",
      "Epoch 133, Loss: 0.0005319394404068589\n",
      "Epoch 134, Loss: 0.0005270843976177275\n",
      "Epoch 135, Loss: 0.0005223187035880983\n",
      "Epoch 136, Loss: 0.00051788060227409\n",
      "Epoch 137, Loss: 0.0005133828963153064\n",
      "Epoch 138, Loss: 0.000508855446241796\n",
      "Epoch 139, Loss: 0.0005042386474087834\n",
      "Epoch 140, Loss: 0.000499830290209502\n",
      "Epoch 141, Loss: 0.0004956006305292249\n",
      "Epoch 142, Loss: 0.0004914007149636745\n",
      "Epoch 143, Loss: 0.00048720085760578513\n",
      "Epoch 144, Loss: 0.0004830008838325739\n",
      "Epoch 145, Loss: 0.00047880091005936265\n",
      "Epoch 146, Loss: 0.00047483918024227023\n",
      "Epoch 147, Loss: 0.0004707285261247307\n",
      "Epoch 148, Loss: 0.0004666774475481361\n",
      "Epoch 149, Loss: 0.00046256676432676613\n",
      "Epoch 150, Loss: 0.00045869435416534543\n",
      "Epoch 151, Loss: 0.0004547921707853675\n",
      "Epoch 152, Loss: 0.00045115803368389606\n",
      "Epoch 153, Loss: 0.0004473153967410326\n",
      "Epoch 154, Loss: 0.000443591910880059\n",
      "Epoch 155, Loss: 0.00043986833770759404\n",
      "Epoch 156, Loss: 0.00043626397382467985\n",
      "Epoch 157, Loss: 0.0004325701738707721\n",
      "Epoch 158, Loss: 0.0004289359785616398\n",
      "Epoch 159, Loss: 0.00042554002720862627\n",
      "Epoch 160, Loss: 0.00042214407585561275\n",
      "Epoch 161, Loss: 0.00041871838038787246\n",
      "Epoch 162, Loss: 0.0004152032488491386\n",
      "Epoch 163, Loss: 0.00041177752427756786\n",
      "Epoch 164, Loss: 0.00040844109025783837\n",
      "Epoch 165, Loss: 0.0004053132142871618\n",
      "Epoch 166, Loss: 0.0004019768093712628\n",
      "Epoch 167, Loss: 0.00039875952643342316\n",
      "Epoch 168, Loss: 0.00039557204581797123\n",
      "Epoch 169, Loss: 0.000392473884858191\n",
      "Epoch 170, Loss: 0.00038922677049413323\n",
      "Epoch 171, Loss: 0.00038621798739768565\n",
      "Epoch 172, Loss: 0.00038317940197885036\n",
      "Epoch 173, Loss: 0.0003801408165600151\n",
      "Epoch 174, Loss: 0.00037728092866018414\n",
      "Epoch 175, Loss: 0.0003743317211046815\n",
      "Epoch 176, Loss: 0.0003714718041010201\n",
      "Epoch 177, Loss: 0.00036849273601546884\n",
      "Epoch 178, Loss: 0.0003656328481156379\n",
      "Epoch 179, Loss: 0.00036283250665292144\n",
      "Epoch 180, Loss: 0.00036009171162731946\n",
      "Epoch 181, Loss: 0.0003573807771317661\n",
      "Epoch 182, Loss: 0.0003546697844285518\n",
      "Epoch 183, Loss: 0.0003519289894029498\n",
      "Epoch 184, Loss: 0.00034930737456306815\n",
      "Epoch 185, Loss: 0.0003465963527560234\n",
      "Epoch 186, Loss: 0.0003439747088123113\n",
      "Epoch 187, Loss: 0.0003414722450543195\n",
      "Epoch 188, Loss: 0.0003388506011106074\n",
      "Epoch 189, Loss: 0.00033634810824878514\n",
      "Epoch 190, Loss: 0.00033387535950168967\n",
      "Epoch 191, Loss: 0.00033134306431747973\n",
      "Epoch 192, Loss: 0.0003289597516413778\n",
      "Epoch 193, Loss: 0.0003265764098614454\n",
      "Epoch 194, Loss: 0.0003241930389776826\n",
      "Epoch 195, Loss: 0.00032180966809391975\n",
      "Epoch 196, Loss: 0.00031945607042871416\n",
      "Epoch 197, Loss: 0.00031716207740828395\n",
      "Epoch 198, Loss: 0.0003148085088469088\n",
      "Epoch 199, Loss: 0.0003125740622635931\n",
      "Epoch 200, Loss: 0.00031028006924316287\n",
      "Epoch 201, Loss: 0.0003081051982007921\n",
      "Epoch 202, Loss: 0.00030590055393986404\n",
      "Epoch 203, Loss: 0.000303695909678936\n",
      "Epoch 204, Loss: 0.0003014614339917898\n",
      "Epoch 205, Loss: 0.0002994652895722538\n",
      "Epoch 206, Loss: 0.0002973202208522707\n",
      "Epoch 207, Loss: 0.0002952049544546753\n",
      "Epoch 208, Loss: 0.0002932088100351393\n",
      "Epoch 209, Loss: 0.0002911530900746584\n",
      "Epoch 210, Loss: 0.00028912717243656516\n",
      "Epoch 211, Loss: 0.00028713102801702917\n",
      "Epoch 212, Loss: 0.00028513488359749317\n",
      "Epoch 213, Loss: 0.0002831387100741267\n",
      "Epoch 214, Loss: 0.00028111276333220303\n",
      "Epoch 215, Loss: 0.0002791761653497815\n",
      "Epoch 216, Loss: 0.0002773289743345231\n",
      "Epoch 217, Loss: 0.0002754520101007074\n",
      "Epoch 218, Loss: 0.0002735154121182859\n",
      "Epoch 219, Loss: 0.00027172782574780285\n",
      "Epoch 220, Loss: 0.00026985080330632627\n",
      "Epoch 221, Loss: 0.0002680035831872374\n",
      "Epoch 222, Loss: 0.00026627551415003836\n",
      "Epoch 223, Loss: 0.0002644879277795553\n",
      "Epoch 224, Loss: 0.000262789661064744\n",
      "Epoch 225, Loss: 0.00026106162113137543\n",
      "Epoch 226, Loss: 0.00025936332531273365\n",
      "Epoch 227, Loss: 0.00025769486092031\n",
      "Epoch 228, Loss: 0.00025596682098694146\n",
      "Epoch 229, Loss: 0.0002541791764087975\n",
      "Epoch 230, Loss: 0.00025251065380871296\n",
      "Epoch 231, Loss: 0.000250871991738677\n",
      "Epoch 232, Loss: 0.00024923330056481063\n",
      "Epoch 233, Loss: 0.0002476243826095015\n",
      "Epoch 234, Loss: 0.0002460154937580228\n",
      "Epoch 235, Loss: 0.00024443637812510133\n",
      "Epoch 236, Loss: 0.00024288706481456757\n",
      "Epoch 237, Loss: 0.00024133773695211858\n",
      "Epoch 238, Loss: 0.00023975862131919712\n",
      "Epoch 239, Loss: 0.00023832847364246845\n",
      "Epoch 240, Loss: 0.0002367195556871593\n",
      "Epoch 241, Loss: 0.00023525962023995817\n",
      "Epoch 242, Loss: 0.00023376985336653888\n",
      "Epoch 243, Loss: 0.00023236952256411314\n",
      "Epoch 244, Loss: 0.00023084995336830616\n",
      "Epoch 245, Loss: 0.0002294198056915775\n",
      "Epoch 246, Loss: 0.00022804923355579376\n",
      "Epoch 247, Loss: 0.00022664887364953756\n",
      "Epoch 248, Loss: 0.00022515909222420305\n",
      "Epoch 249, Loss: 0.00022375871776603162\n",
      "Epoch 250, Loss: 0.00022238813107833266\n",
      "Epoch 251, Loss: 0.00022113672457635403\n",
      "Epoch 252, Loss: 0.00021970656234771013\n",
      "Epoch 253, Loss: 0.00021836577798239887\n",
      "Epoch 254, Loss: 0.00021705476683564484\n",
      "Epoch 255, Loss: 0.00021574378479272127\n",
      "Epoch 256, Loss: 0.00021443278819788247\n",
      "Epoch 257, Loss: 0.00021312177705112845\n",
      "Epoch 258, Loss: 0.00021184058277867734\n",
      "Epoch 259, Loss: 0.00021058914717286825\n",
      "Epoch 260, Loss: 0.00020930793834850192\n",
      "Epoch 261, Loss: 0.0002079969272017479\n",
      "Epoch 262, Loss: 0.00020671571837738156\n",
      "Epoch 263, Loss: 0.00020555368973873556\n",
      "Epoch 264, Loss: 0.00020436184422578663\n",
      "Epoch 265, Loss: 0.00020317001326475292\n",
      "Epoch 266, Loss: 0.00020194837998133153\n",
      "Epoch 267, Loss: 0.00020075654902029783\n",
      "Epoch 268, Loss: 0.00019962430815212429\n",
      "Epoch 269, Loss: 0.0001983728725463152\n",
      "Epoch 270, Loss: 0.00019721081480383873\n",
      "Epoch 271, Loss: 0.0001960785739356652\n",
      "Epoch 272, Loss: 0.00019491653074510396\n",
      "Epoch 273, Loss: 0.0001937842753250152\n",
      "Epoch 274, Loss: 0.00019271160999778658\n",
      "Epoch 275, Loss: 0.00019157934002578259\n",
      "Epoch 276, Loss: 0.00019044709915760905\n",
      "Epoch 277, Loss: 0.0001894042216008529\n",
      "Epoch 278, Loss: 0.00018830176850315183\n",
      "Epoch 279, Loss: 0.00018725887639448047\n",
      "Epoch 280, Loss: 0.00018621602794155478\n",
      "Epoch 281, Loss: 0.00018517315038479865\n",
      "Epoch 282, Loss: 0.0001841302728280425\n",
      "Epoch 283, Loss: 0.0001830278051784262\n",
      "Epoch 284, Loss: 0.00018204453226644546\n",
      "Epoch 285, Loss: 0.00018109103257302195\n",
      "Epoch 286, Loss: 0.00017998856492340565\n",
      "Epoch 287, Loss: 0.00017903506522998214\n",
      "Epoch 288, Loss: 0.0001780517923180014\n",
      "Epoch 289, Loss: 0.00017706849030219018\n",
      "Epoch 290, Loss: 0.00017608521739020944\n",
      "Epoch 291, Loss: 0.0001751615054672584\n",
      "Epoch 292, Loss: 0.00017423780809622258\n",
      "Epoch 293, Loss: 0.0001732247183099389\n",
      "Epoch 294, Loss: 0.00017227123316843063\n",
      "Epoch 295, Loss: 0.00017125812883023173\n",
      "Epoch 296, Loss: 0.00017039402155205607\n",
      "Epoch 297, Loss: 0.00016947030962910503\n",
      "Epoch 298, Loss: 0.00016848702216520905\n",
      "Epoch 299, Loss: 0.00016762290033511817\n",
      "Epoch 300, Loss: 0.00016669918841216713\n",
      "Epoch 301, Loss: 0.00016583508113399148\n",
      "Epoch 302, Loss: 0.00016494117153342813\n",
      "Epoch 303, Loss: 0.00016404726193286479\n",
      "Epoch 304, Loss: 0.0001631831401027739\n",
      "Epoch 305, Loss: 0.00016231901827268302\n",
      "Epoch 306, Loss: 0.0001614251232240349\n",
      "Epoch 307, Loss: 0.0001605907891644165\n",
      "Epoch 308, Loss: 0.00015972668188624084\n",
      "Epoch 309, Loss: 0.00015889236237853765\n",
      "Epoch 310, Loss: 0.00015808783064130694\n",
      "Epoch 311, Loss: 0.00015722370881121606\n",
      "Epoch 312, Loss: 0.00015641917707398534\n",
      "Epoch 313, Loss: 0.00015561465988866985\n",
      "Epoch 314, Loss: 0.0001548101135995239\n",
      "Epoch 315, Loss: 0.0001540055964142084\n",
      "Epoch 316, Loss: 0.00015317126235459\n",
      "Epoch 317, Loss: 0.00015242633526213467\n",
      "Epoch 318, Loss: 0.00015162180352490395\n",
      "Epoch 319, Loss: 0.0001508768618805334\n",
      "Epoch 320, Loss: 0.00015007233014330268\n",
      "Epoch 321, Loss: 0.00014932738849893212\n",
      "Epoch 322, Loss: 0.0001485526590840891\n",
      "Epoch 323, Loss: 0.0001478673075325787\n",
      "Epoch 324, Loss: 0.0001470925926696509\n",
      "Epoch 325, Loss: 0.00014634763647336513\n",
      "Epoch 326, Loss: 0.00014560269482899457\n",
      "Epoch 327, Loss: 0.00014485775318462402\n",
      "Epoch 328, Loss: 0.00014417240163311362\n",
      "Epoch 329, Loss: 0.0001433678698958829\n",
      "Epoch 330, Loss: 0.00014265271602198482\n",
      "Epoch 331, Loss: 0.0001418183819623664\n",
      "Epoch 332, Loss: 0.00014089462638366967\n",
      "Epoch 333, Loss: 0.0001403284550178796\n",
      "Epoch 334, Loss: 0.00013931530702393502\n",
      "Epoch 335, Loss: 0.00013830217358190566\n",
      "Epoch 336, Loss: 0.00013734863023273647\n",
      "Epoch 337, Loss: 0.00013645467697642744\n",
      "Epoch 338, Loss: 0.0001355607237201184\n",
      "Epoch 339, Loss: 0.0001345177588518709\n",
      "Epoch 340, Loss: 0.00013332581147551537\n",
      "Epoch 341, Loss: 0.00013201465480960906\n",
      "Epoch 342, Loss: 0.00013067369582131505\n",
      "Epoch 343, Loss: 0.0001296008995268494\n",
      "Epoch 344, Loss: 0.00012831951607950032\n",
      "Epoch 345, Loss: 0.00012662098743021488\n",
      "Epoch 346, Loss: 0.0001253694063052535\n",
      "Epoch 347, Loss: 0.00012408803740981966\n",
      "Epoch 348, Loss: 0.00012322381371632218\n",
      "Epoch 349, Loss: 0.00012188283290015534\n",
      "Epoch 350, Loss: 0.00012033329403493553\n",
      "Epoch 351, Loss: 0.0001192009003716521\n",
      "Epoch 352, Loss: 0.00011794930469477549\n",
      "Epoch 353, Loss: 0.0001167275186162442\n",
      "Epoch 354, Loss: 0.00011547593749128282\n",
      "Epoch 355, Loss: 0.00011428393918322399\n",
      "Epoch 356, Loss: 0.00011324093793518841\n",
      "Epoch 357, Loss: 0.0001120489469030872\n",
      "Epoch 358, Loss: 0.00011097617243649438\n",
      "Epoch 359, Loss: 0.00011049934255424887\n",
      "Epoch 360, Loss: 0.00010921798821073025\n",
      "Epoch 361, Loss: 0.00010841339826583862\n",
      "Epoch 362, Loss: 0.00010754921822808683\n",
      "Epoch 363, Loss: 0.00010677443060558289\n",
      "Epoch 364, Loss: 0.0001058804482454434\n",
      "Epoch 365, Loss: 0.00010510566062293947\n",
      "Epoch 366, Loss: 0.00010436066804686561\n",
      "Epoch 367, Loss: 0.00010358588770031929\n",
      "Epoch 368, Loss: 0.00010275150998495519\n",
      "Epoch 369, Loss: 0.00010209592437604442\n",
      "Epoch 370, Loss: 0.00010141053644474596\n",
      "Epoch 371, Loss: 0.00010069535346701741\n",
      "Epoch 372, Loss: 0.00010027815005742013\n",
      "Epoch 373, Loss: 9.956298163160682e-05\n",
      "Epoch 374, Loss: 9.887760825222358e-05\n",
      "Epoch 375, Loss: 9.828163456404582e-05\n",
      "Epoch 376, Loss: 9.771546319825575e-05\n",
      "Epoch 377, Loss: 9.720888192532584e-05\n",
      "Epoch 378, Loss: 9.661290823714808e-05\n",
      "Epoch 379, Loss: 9.610632696421817e-05\n",
      "Epoch 380, Loss: 9.559973841533065e-05\n",
      "Epoch 381, Loss: 9.497396240476519e-05\n",
      "Epoch 382, Loss: 9.449718345422298e-05\n",
      "Epoch 383, Loss: 9.39310048124753e-05\n",
      "Epoch 384, Loss: 9.342441626358777e-05\n",
      "Epoch 385, Loss: 9.294764458900318e-05\n",
      "Epoch 386, Loss: 9.23516636248678e-05\n",
      "Epoch 387, Loss: 9.190467972075567e-05\n",
      "Epoch 388, Loss: 9.139809117186815e-05\n",
      "Epoch 389, Loss: 9.089150989893824e-05\n",
      "Epoch 390, Loss: 9.038492862600833e-05\n",
      "Epoch 391, Loss: 8.990814967546612e-05\n",
      "Epoch 392, Loss: 8.943135617300868e-05\n",
      "Epoch 393, Loss: 8.895457722246647e-05\n",
      "Epoch 394, Loss: 8.853738836478442e-05\n",
      "Epoch 395, Loss: 8.800100476946682e-05\n",
      "Epoch 396, Loss: 8.75540281413123e-05\n",
      "Epoch 397, Loss: 8.704743959242478e-05\n",
      "Epoch 398, Loss: 8.657065336592495e-05\n",
      "Epoch 399, Loss: 8.603427704656497e-05\n",
      "Epoch 400, Loss: 8.555749082006514e-05\n",
      "Epoch 401, Loss: 8.519990660715848e-05\n",
      "Epoch 402, Loss: 8.454433555016294e-05\n",
      "Epoch 403, Loss: 8.400794467888772e-05\n",
      "Epoch 404, Loss: 8.356096805073321e-05\n",
      "Epoch 405, Loss: 8.311398414662108e-05\n",
      "Epoch 406, Loss: 8.260739559773356e-05\n",
      "Epoch 407, Loss: 8.210081432480365e-05\n",
      "Epoch 408, Loss: 8.162403537426144e-05\n",
      "Epoch 409, Loss: 8.11770441941917e-05\n",
      "Epoch 410, Loss: 8.067046292126179e-05\n",
      "Epoch 411, Loss: 8.019367669476196e-05\n",
      "Epoch 412, Loss: 7.971689046826214e-05\n",
      "Epoch 413, Loss: 7.929970161058009e-05\n",
      "Epoch 414, Loss: 7.885271043051034e-05\n",
      "Epoch 415, Loss: 7.834612915758044e-05\n",
      "Epoch 416, Loss: 7.786934293108061e-05\n",
      "Epoch 417, Loss: 7.748194911982864e-05\n",
      "Epoch 418, Loss: 7.697536057094112e-05\n",
      "Epoch 419, Loss: 7.661776908207685e-05\n",
      "Epoch 420, Loss: 7.614099013153464e-05\n",
      "Epoch 421, Loss: 7.572379399789497e-05\n",
      "Epoch 422, Loss: 7.530660514021292e-05\n",
      "Epoch 423, Loss: 7.485961396014318e-05\n",
      "Epoch 424, Loss: 7.447222742484882e-05\n",
      "Epoch 425, Loss: 7.408483361359686e-05\n",
      "Epoch 426, Loss: 7.375703717116266e-05\n",
      "Epoch 427, Loss: 7.325045589823276e-05\n",
      "Epoch 428, Loss: 7.292265945579857e-05\n",
      "Epoch 429, Loss: 7.247566827572882e-05\n",
      "Epoch 430, Loss: 7.211807678686455e-05\n",
      "Epoch 431, Loss: 7.173068297561258e-05\n",
      "Epoch 432, Loss: 7.134328916436061e-05\n",
      "Epoch 433, Loss: 7.098569767549634e-05\n",
      "Epoch 434, Loss: 7.062809891067445e-05\n",
      "Epoch 435, Loss: 7.02407123753801e-05\n",
      "Epoch 436, Loss: 6.988312088651583e-05\n",
      "Epoch 437, Loss: 6.955532444408163e-05\n",
      "Epoch 438, Loss: 6.919773295521736e-05\n",
      "Epoch 439, Loss: 6.886993651278317e-05\n",
      "Epoch 440, Loss: 6.854214007034898e-05\n",
      "Epoch 441, Loss: 6.812494393670931e-05\n",
      "Epoch 442, Loss: 6.782694981666282e-05\n",
      "Epoch 443, Loss: 6.743955600541085e-05\n",
      "Epoch 444, Loss: 6.711175228701904e-05\n",
      "Epoch 445, Loss: 6.678396312054247e-05\n",
      "Epoch 446, Loss: 6.645616667810827e-05\n",
      "Epoch 447, Loss: 6.609856791328639e-05\n",
      "Epoch 448, Loss: 6.580057379323989e-05\n",
      "Epoch 449, Loss: 6.5442975028418e-05\n",
      "Epoch 450, Loss: 6.523438059957698e-05\n",
      "Epoch 451, Loss: 6.484698678832501e-05\n",
      "Epoch 452, Loss: 6.45489853923209e-05\n",
      "Epoch 453, Loss: 6.416159158106893e-05\n",
      "Epoch 454, Loss: 6.39529898762703e-05\n",
      "Epoch 455, Loss: 6.36251934338361e-05\n",
      "Epoch 456, Loss: 6.326759466901422e-05\n",
      "Epoch 457, Loss: 6.299940287135541e-05\n",
      "Epoch 458, Loss: 6.27014014753513e-05\n",
      "Epoch 459, Loss: 6.243320240173489e-05\n",
      "Epoch 460, Loss: 6.21054059593007e-05\n",
      "Epoch 461, Loss: 6.183719960972667e-05\n",
      "Epoch 462, Loss: 6.153920548968017e-05\n",
      "Epoch 463, Loss: 6.127100641606376e-05\n",
      "Epoch 464, Loss: 6.097301229601726e-05\n",
      "Epoch 465, Loss: 6.067501090001315e-05\n",
      "Epoch 466, Loss: 6.0406811826396734e-05\n",
      "Epoch 467, Loss: 6.0168415075168014e-05\n",
      "Epoch 468, Loss: 5.984061135677621e-05\n",
      "Epoch 469, Loss: 5.960221460554749e-05\n",
      "Epoch 470, Loss: 5.936381421633996e-05\n",
      "Epoch 471, Loss: 5.915521614952013e-05\n",
      "Epoch 472, Loss: 5.8797613746719435e-05\n",
      "Epoch 473, Loss: 5.855921335751191e-05\n",
      "Epoch 474, Loss: 5.8261215599486604e-05\n",
      "Epoch 475, Loss: 5.802282248623669e-05\n",
      "Epoch 476, Loss: 5.775461613666266e-05\n",
      "Epoch 477, Loss: 5.757581675425172e-05\n",
      "Epoch 478, Loss: 5.73076176806353e-05\n",
      "Epoch 479, Loss: 5.706922092940658e-05\n",
      "Epoch 480, Loss: 5.674141721101478e-05\n",
      "Epoch 481, Loss: 5.6562617828603834e-05\n",
      "Epoch 482, Loss: 5.626461643259972e-05\n",
      "Epoch 483, Loss: 5.605601836577989e-05\n",
      "Epoch 484, Loss: 5.584741666098125e-05\n",
      "Epoch 485, Loss: 5.557921394938603e-05\n",
      "Epoch 486, Loss: 5.528121255338192e-05\n",
      "Epoch 487, Loss: 5.5102413170970976e-05\n",
      "Epoch 488, Loss: 5.483421409735456e-05\n",
      "Epoch 489, Loss: 5.462561239255592e-05\n",
      "Epoch 490, Loss: 5.438720836536959e-05\n",
      "Epoch 491, Loss: 5.414881161414087e-05\n",
      "Epoch 492, Loss: 5.394020990934223e-05\n",
      "Epoch 493, Loss: 5.373160820454359e-05\n",
      "Epoch 494, Loss: 5.3493204177357256e-05\n",
      "Epoch 495, Loss: 5.3254807426128536e-05\n",
      "Epoch 496, Loss: 5.307600076775998e-05\n",
      "Epoch 497, Loss: 5.286740270094015e-05\n",
      "Epoch 498, Loss: 5.253959898254834e-05\n",
      "Epoch 499, Loss: 5.236079596215859e-05\n",
      "Epoch 500, Loss: 5.218199657974765e-05\n",
      "Epoch 501, Loss: 5.1913793868152425e-05\n",
      "Epoch 502, Loss: 5.176479317015037e-05\n",
      "Epoch 503, Loss: 5.152639278094284e-05\n",
      "Epoch 504, Loss: 5.134758976055309e-05\n",
      "Epoch 505, Loss: 5.1079390686936677e-05\n",
      "Epoch 506, Loss: 5.0900587666546926e-05\n",
      "Epoch 507, Loss: 5.0691989599727094e-05\n",
      "Epoch 508, Loss: 5.048338061897084e-05\n",
      "Epoch 509, Loss: 5.03045812365599e-05\n",
      "Epoch 510, Loss: 5.009597953176126e-05\n",
      "Epoch 511, Loss: 4.988737782696262e-05\n",
      "Epoch 512, Loss: 4.970857480657287e-05\n",
      "Epoch 513, Loss: 4.949997310177423e-05\n",
      "Epoch 514, Loss: 4.932117008138448e-05\n",
      "Epoch 515, Loss: 4.911256837658584e-05\n",
      "Epoch 516, Loss: 4.89337689941749e-05\n",
      "Epoch 517, Loss: 4.872516365139745e-05\n",
      "Epoch 518, Loss: 4.854636426898651e-05\n",
      "Epoch 519, Loss: 4.833775892620906e-05\n",
      "Epoch 520, Loss: 4.815895590581931e-05\n",
      "Epoch 521, Loss: 4.798015652340837e-05\n",
      "Epoch 522, Loss: 4.7741752496222034e-05\n",
      "Epoch 523, Loss: 4.759275179821998e-05\n",
      "Epoch 524, Loss: 4.741394877783023e-05\n",
      "Epoch 525, Loss: 4.7264944441849366e-05\n",
      "Epoch 526, Loss: 4.6996741730254143e-05\n",
      "Epoch 527, Loss: 4.6817935071885586e-05\n",
      "Epoch 528, Loss: 4.663913568947464e-05\n",
      "Epoch 529, Loss: 4.64603363070637e-05\n",
      "Epoch 530, Loss: 4.634113429347053e-05\n",
      "Epoch 531, Loss: 4.613252531271428e-05\n",
      "Epoch 532, Loss: 4.601332693709992e-05\n",
      "Epoch 533, Loss: 4.580472159432247e-05\n",
      "Epoch 534, Loss: 4.562592221191153e-05\n",
      "Epoch 535, Loss: 4.544711555354297e-05\n",
      "Epoch 536, Loss: 4.5298114855540916e-05\n",
      "Epoch 537, Loss: 4.514911415753886e-05\n",
      "Epoch 538, Loss: 4.491071013035253e-05\n",
      "Epoch 539, Loss: 4.4761705794371665e-05\n",
      "Epoch 540, Loss: 4.461270509636961e-05\n",
      "Epoch 541, Loss: 4.4463704398367554e-05\n",
      "Epoch 542, Loss: 4.431470006238669e-05\n",
      "Epoch 543, Loss: 4.407629239722155e-05\n",
      "Epoch 544, Loss: 4.3986896343994886e-05\n",
      "Epoch 545, Loss: 4.380808968562633e-05\n",
      "Epoch 546, Loss: 4.3659085349645466e-05\n",
      "Epoch 547, Loss: 4.351008465164341e-05\n",
      "Epoch 548, Loss: 4.3361083953641355e-05\n",
      "Epoch 549, Loss: 4.321207961766049e-05\n",
      "Epoch 550, Loss: 4.306307891965844e-05\n",
      "Epoch 551, Loss: 4.2824671254493296e-05\n",
      "Epoch 552, Loss: 4.2735271563287824e-05\n",
      "Epoch 553, Loss: 4.255646490491927e-05\n",
      "Epoch 554, Loss: 4.240746420691721e-05\n",
      "Epoch 555, Loss: 4.222866118652746e-05\n",
      "Epoch 556, Loss: 4.2109459172934294e-05\n",
      "Epoch 557, Loss: 4.196045483695343e-05\n",
      "Epoch 558, Loss: 4.1811454138951376e-05\n",
      "Epoch 559, Loss: 4.169225212535821e-05\n",
      "Epoch 560, Loss: 4.151344546698965e-05\n",
      "Epoch 561, Loss: 4.1364444768987596e-05\n",
      "Epoch 562, Loss: 4.1215436795027927e-05\n",
      "Epoch 563, Loss: 4.1096238419413567e-05\n",
      "Epoch 564, Loss: 4.0947234083432704e-05\n",
      "Epoch 565, Loss: 4.0768427425064147e-05\n",
      "Epoch 566, Loss: 4.061942672706209e-05\n",
      "Epoch 567, Loss: 4.0470426029060036e-05\n",
      "Epoch 568, Loss: 4.035122401546687e-05\n",
      "Epoch 569, Loss: 4.0202219679486006e-05\n",
      "Epoch 570, Loss: 4.008301766589284e-05\n",
      "Epoch 571, Loss: 3.993400969193317e-05\n",
      "Epoch 572, Loss: 3.978500535595231e-05\n",
      "Epoch 573, Loss: 3.966580334235914e-05\n",
      "Epoch 574, Loss: 3.951679900637828e-05\n",
      "Epoch 575, Loss: 3.9367794670397416e-05\n",
      "Epoch 576, Loss: 3.924859265680425e-05\n",
      "Epoch 577, Loss: 3.90697896364145e-05\n",
      "Epoch 578, Loss: 3.895058762282133e-05\n",
      "Epoch 579, Loss: 3.886118429363705e-05\n",
      "Epoch 580, Loss: 3.86823812732473e-05\n",
      "Epoch 581, Loss: 3.859298158204183e-05\n",
      "Epoch 582, Loss: 3.844397360808216e-05\n",
      "Epoch 583, Loss: 3.8294972910080105e-05\n",
      "Epoch 584, Loss: 3.820556594291702e-05\n",
      "Epoch 585, Loss: 3.8056565244914964e-05\n",
      "Epoch 586, Loss: 3.793735959334299e-05\n",
      "Epoch 587, Loss: 3.781815757974982e-05\n",
      "Epoch 588, Loss: 3.7728754250565544e-05\n",
      "Epoch 589, Loss: 3.754995123017579e-05\n",
      "Epoch 590, Loss: 3.7430749216582626e-05\n",
      "Epoch 591, Loss: 3.7281744880601764e-05\n",
      "Epoch 592, Loss: 3.71625428670086e-05\n",
      "Epoch 593, Loss: 3.707313953782432e-05\n",
      "Epoch 594, Loss: 3.6924135201843455e-05\n",
      "Epoch 595, Loss: 3.680492955027148e-05\n",
      "Epoch 596, Loss: 3.671552985906601e-05\n",
      "Epoch 597, Loss: 3.6596324207494035e-05\n",
      "Epoch 598, Loss: 3.644732350949198e-05\n",
      "Epoch 599, Loss: 3.6328117857920006e-05\n",
      "Epoch 600, Loss: 3.623871452873573e-05\n",
      "Epoch 601, Loss: 3.611951251514256e-05\n",
      "Epoch 602, Loss: 3.600031050154939e-05\n",
      "Epoch 603, Loss: 3.588110484997742e-05\n",
      "Epoch 604, Loss: 3.5821503843180835e-05\n",
      "Epoch 605, Loss: 3.561289850040339e-05\n",
      "Epoch 606, Loss: 3.5493692848831415e-05\n",
      "Epoch 607, Loss: 3.537449083523825e-05\n",
      "Epoch 608, Loss: 3.528508750605397e-05\n",
      "Epoch 609, Loss: 3.51658854924608e-05\n",
      "Epoch 610, Loss: 3.507648216327652e-05\n",
      "Epoch 611, Loss: 3.492747782729566e-05\n",
      "Epoch 612, Loss: 3.483807449811138e-05\n",
      "Epoch 613, Loss: 3.4718872484518215e-05\n",
      "Epoch 614, Loss: 3.4629469155333936e-05\n",
      "Epoch 615, Loss: 3.454006582614966e-05\n",
      "Epoch 616, Loss: 3.445066249696538e-05\n",
      "Epoch 617, Loss: 3.4301658160984516e-05\n",
      "Epoch 618, Loss: 3.418245614739135e-05\n",
      "Epoch 619, Loss: 3.412285150261596e-05\n",
      "Epoch 620, Loss: 3.400364948902279e-05\n",
      "Epoch 621, Loss: 3.3884447475429624e-05\n",
      "Epoch 622, Loss: 3.3795044146245345e-05\n",
      "Epoch 623, Loss: 3.361623748787679e-05\n",
      "Epoch 624, Loss: 3.352683415869251e-05\n",
      "Epoch 625, Loss: 3.343743082950823e-05\n",
      "Epoch 626, Loss: 3.331822881591506e-05\n",
      "Epoch 627, Loss: 3.319902316434309e-05\n",
      "Epoch 628, Loss: 3.310961983515881e-05\n",
      "Epoch 629, Loss: 3.307982115074992e-05\n",
      "Epoch 630, Loss: 3.2901014492381364e-05\n",
      "Epoch 631, Loss: 3.281160752521828e-05\n",
      "Epoch 632, Loss: 3.2722207834012806e-05\n",
      "Epoch 633, Loss: 3.260300582041964e-05\n",
      "Epoch 634, Loss: 3.254340117564425e-05\n",
      "Epoch 635, Loss: 3.242419916205108e-05\n",
      "Epoch 636, Loss: 3.2334792194887996e-05\n",
      "Epoch 637, Loss: 3.2245392503682524e-05\n",
      "Epoch 638, Loss: 3.2155989174498245e-05\n",
      "Epoch 639, Loss: 3.203678352292627e-05\n",
      "Epoch 640, Loss: 3.1917581509333104e-05\n",
      "Epoch 641, Loss: 3.185797686455771e-05\n",
      "Epoch 642, Loss: 3.1738774850964546e-05\n",
      "Epoch 643, Loss: 3.1589770514983684e-05\n",
      "Epoch 644, Loss: 3.155996819259599e-05\n",
      "Epoch 645, Loss: 3.147056486341171e-05\n",
      "Epoch 646, Loss: 3.1410963856615126e-05\n",
      "Epoch 647, Loss: 3.1261959520634264e-05\n",
      "Epoch 648, Loss: 3.120235487585887e-05\n",
      "Epoch 649, Loss: 3.1112951546674594e-05\n",
      "Epoch 650, Loss: 3.1023548217490315e-05\n",
      "Epoch 651, Loss: 3.090434620389715e-05\n",
      "Epoch 652, Loss: 3.081494287471287e-05\n",
      "Epoch 653, Loss: 3.0755341867916286e-05\n",
      "Epoch 654, Loss: 3.06659349007532e-05\n",
      "Epoch 655, Loss: 3.0516930564772338e-05\n",
      "Epoch 656, Loss: 3.042752723558806e-05\n",
      "Epoch 657, Loss: 3.0367926228791475e-05\n",
      "Epoch 658, Loss: 3.0278522899607196e-05\n",
      "Epoch 659, Loss: 3.0189119570422918e-05\n",
      "Epoch 660, Loss: 3.009971624123864e-05\n",
      "Epoch 661, Loss: 3.004011341545265e-05\n",
      "Epoch 662, Loss: 2.9920911401859485e-05\n",
      "Epoch 663, Loss: 2.9861306757084094e-05\n",
      "Epoch 664, Loss: 2.9742102924501523e-05\n",
      "Epoch 665, Loss: 2.968250191770494e-05\n",
      "Epoch 666, Loss: 2.9593096769531257e-05\n",
      "Epoch 667, Loss: 2.9533495762734674e-05\n",
      "Epoch 668, Loss: 2.9414291930152103e-05\n",
      "Epoch 669, Loss: 2.9354689104366116e-05\n",
      "Epoch 670, Loss: 2.926528759417124e-05\n",
      "Epoch 671, Loss: 2.911627962021157e-05\n",
      "Epoch 672, Loss: 2.911627962021157e-05\n",
      "Epoch 673, Loss: 2.8997073968639597e-05\n",
      "Epoch 674, Loss: 2.8937472961843014e-05\n",
      "Epoch 675, Loss: 2.8788466806872748e-05\n",
      "Epoch 676, Loss: 2.8758666303474456e-05\n",
      "Epoch 677, Loss: 2.8609658329514787e-05\n",
      "Epoch 678, Loss: 2.857985600712709e-05\n",
      "Epoch 679, Loss: 2.843085167114623e-05\n",
      "Epoch 680, Loss: 2.8401049348758534e-05\n",
      "Epoch 681, Loss: 2.834144834196195e-05\n",
      "Epoch 682, Loss: 2.8222242690389976e-05\n",
      "Epoch 683, Loss: 2.8132839361205697e-05\n",
      "Epoch 684, Loss: 2.8103038857807405e-05\n",
      "Epoch 685, Loss: 2.7983835025224835e-05\n",
      "Epoch 686, Loss: 2.7924230380449444e-05\n",
      "Epoch 687, Loss: 2.783482887025457e-05\n",
      "Epoch 688, Loss: 2.777522604446858e-05\n",
      "Epoch 689, Loss: 2.7715625037671998e-05\n",
      "Epoch 690, Loss: 2.765602221188601e-05\n",
      "Epoch 691, Loss: 2.753681837930344e-05\n",
      "Epoch 692, Loss: 2.747721373452805e-05\n",
      "Epoch 693, Loss: 2.7417612727731466e-05\n",
      "Epoch 694, Loss: 2.7298408895148896e-05\n",
      "Epoch 695, Loss: 2.72686065727612e-05\n",
      "Epoch 696, Loss: 2.717920324357692e-05\n",
      "Epoch 697, Loss: 2.7059995773015544e-05\n",
      "Epoch 698, Loss: 2.700039476621896e-05\n",
      "Epoch 699, Loss: 2.697059426282067e-05\n",
      "Epoch 700, Loss: 2.6851390430238098e-05\n",
      "Epoch 701, Loss: 2.6821588107850403e-05\n",
      "Epoch 702, Loss: 2.6702386094257236e-05\n",
      "Epoch 703, Loss: 2.6642781449481845e-05\n",
      "Epoch 704, Loss: 2.6553378120297566e-05\n",
      "Epoch 705, Loss: 2.652357579790987e-05\n",
      "Epoch 706, Loss: 2.6434172468725592e-05\n",
      "Epoch 707, Loss: 2.64043719653273e-05\n",
      "Epoch 708, Loss: 2.6285164494765922e-05\n",
      "Epoch 709, Loss: 2.6195761165581644e-05\n",
      "Epoch 710, Loss: 2.613616015878506e-05\n",
      "Epoch 711, Loss: 2.6076557332999073e-05\n",
      "Epoch 712, Loss: 2.6076557332999073e-05\n",
      "Epoch 713, Loss: 2.5897748855641112e-05\n",
      "Epoch 714, Loss: 2.5897748855641112e-05\n",
      "Epoch 715, Loss: 2.5808345526456833e-05\n",
      "Epoch 716, Loss: 2.574874451966025e-05\n",
      "Epoch 717, Loss: 2.568913987488486e-05\n",
      "Epoch 718, Loss: 2.5569934223312885e-05\n",
      "Epoch 719, Loss: 2.5540135538903996e-05\n",
      "Epoch 720, Loss: 2.5420929887332022e-05\n",
      "Epoch 721, Loss: 2.5420929887332022e-05\n",
      "Epoch 722, Loss: 2.5301724235760048e-05\n",
      "Epoch 723, Loss: 2.5271923732361756e-05\n",
      "Epoch 724, Loss: 2.521232090657577e-05\n",
      "Epoch 725, Loss: 2.512291757739149e-05\n",
      "Epoch 726, Loss: 2.5063314751605503e-05\n",
      "Epoch 727, Loss: 2.494410910003353e-05\n",
      "Epoch 728, Loss: 2.4944107281044126e-05\n",
      "Epoch 729, Loss: 2.485470577084925e-05\n",
      "Epoch 730, Loss: 2.4765300622675568e-05\n",
      "Epoch 731, Loss: 2.470569779688958e-05\n",
      "Epoch 732, Loss: 2.467589729349129e-05\n",
      "Epoch 733, Loss: 2.4646094971103594e-05\n",
      "Epoch 734, Loss: 2.4526891138521023e-05\n",
      "Epoch 735, Loss: 2.4467288312735036e-05\n",
      "Epoch 736, Loss: 2.440768548694905e-05\n",
      "Epoch 737, Loss: 2.4348082661163062e-05\n",
      "Epoch 738, Loss: 2.4318280338775367e-05\n",
      "Epoch 739, Loss: 2.4199076506192796e-05\n",
      "Epoch 740, Loss: 2.41990783251822e-05\n",
      "Epoch 741, Loss: 2.4109671358019114e-05\n",
      "Epoch 742, Loss: 2.4020268028834835e-05\n",
      "Epoch 743, Loss: 2.3960665203048848e-05\n",
      "Epoch 744, Loss: 2.3901064196252264e-05\n",
      "Epoch 745, Loss: 2.381165904807858e-05\n",
      "Epoch 746, Loss: 2.3781856725690886e-05\n",
      "Epoch 747, Loss: 2.3692453396506608e-05\n",
      "Epoch 748, Loss: 2.3692453396506608e-05\n",
      "Epoch 749, Loss: 2.354344724153634e-05\n",
      "Epoch 750, Loss: 2.3543445422546938e-05\n",
      "Epoch 751, Loss: 2.3513644919148646e-05\n",
      "Epoch 752, Loss: 2.3424241589964367e-05\n",
      "Epoch 753, Loss: 2.3394441086566076e-05\n",
      "Epoch 754, Loss: 2.32752354349941e-05\n",
      "Epoch 755, Loss: 2.3245433112606406e-05\n",
      "Epoch 756, Loss: 2.318583028682042e-05\n",
      "Epoch 757, Loss: 2.309642695763614e-05\n",
      "Epoch 758, Loss: 2.306662645423785e-05\n",
      "Epoch 759, Loss: 2.3036825950839557e-05\n",
      "Epoch 760, Loss: 2.2977221306064166e-05\n",
      "Epoch 761, Loss: 2.291761848027818e-05\n",
      "Epoch 762, Loss: 2.2798412828706205e-05\n",
      "Epoch 763, Loss: 2.2798412828706205e-05\n",
      "Epoch 764, Loss: 2.2798412828706205e-05\n",
      "Epoch 765, Loss: 2.267920717713423e-05\n",
      "Epoch 766, Loss: 2.2619604351348244e-05\n",
      "Epoch 767, Loss: 2.2560001525562257e-05\n",
      "Epoch 768, Loss: 2.2500400518765673e-05\n",
      "Epoch 769, Loss: 2.2440795873990282e-05\n",
      "Epoch 770, Loss: 2.2351392544806004e-05\n",
      "Epoch 771, Loss: 2.2291789719020016e-05\n",
      "Epoch 772, Loss: 2.2291789719020016e-05\n",
      "Epoch 773, Loss: 2.226198739663232e-05\n",
      "Epoch 774, Loss: 2.2172584067448042e-05\n",
      "Epoch 775, Loss: 2.2112981241662055e-05\n",
      "Epoch 776, Loss: 2.2023577912477776e-05\n",
      "Epoch 777, Loss: 2.199377559009008e-05\n",
      "Epoch 778, Loss: 2.1934172764304094e-05\n",
      "Epoch 779, Loss: 2.19043704419164e-05\n",
      "Epoch 780, Loss: 2.184476761613041e-05\n",
      "Epoch 781, Loss: 2.1785166609333828e-05\n",
      "Epoch 782, Loss: 2.1785166609333828e-05\n",
      "Epoch 783, Loss: 2.166595913877245e-05\n",
      "Epoch 784, Loss: 2.163615863537416e-05\n",
      "Epoch 785, Loss: 2.1606356312986463e-05\n",
      "Epoch 786, Loss: 2.1516952983802184e-05\n",
      "Epoch 787, Loss: 2.148715066141449e-05\n",
      "Epoch 788, Loss: 2.1427549654617906e-05\n",
      "Epoch 789, Loss: 2.1367945009842515e-05\n",
      "Epoch 790, Loss: 2.130834400304593e-05\n",
      "Epoch 791, Loss: 2.1278541680658236e-05\n",
      "Epoch 792, Loss: 2.1218937035882846e-05\n",
      "Epoch 793, Loss: 2.1159336029086262e-05\n",
      "Epoch 794, Loss: 2.1129533706698567e-05\n",
      "Epoch 795, Loss: 2.106993088091258e-05\n",
      "Epoch 796, Loss: 2.1010328055126593e-05\n",
      "Epoch 797, Loss: 2.0950725229340605e-05\n",
      "Epoch 798, Loss: 2.083151957776863e-05\n",
      "Epoch 799, Loss: 2.0801717255380936e-05\n",
      "Epoch 800, Loss: 2.0801717255380936e-05\n",
      "Epoch 801, Loss: 2.0712313926196657e-05\n",
      "Epoch 802, Loss: 2.0682511603808962e-05\n",
      "Epoch 803, Loss: 2.0682511603808962e-05\n",
      "Epoch 804, Loss: 2.056330777122639e-05\n",
      "Epoch 805, Loss: 2.056330777122639e-05\n",
      "Epoch 806, Loss: 2.0503703126451e-05\n",
      "Epoch 807, Loss: 2.0414299797266722e-05\n",
      "Epoch 808, Loss: 2.0414299797266722e-05\n",
      "Epoch 809, Loss: 2.035469515249133e-05\n",
      "Epoch 810, Loss: 2.032489464909304e-05\n",
      "Epoch 811, Loss: 2.0265291823307052e-05\n",
      "Epoch 812, Loss: 2.020568717853166e-05\n",
      "Epoch 813, Loss: 2.020568717853166e-05\n",
      "Epoch 814, Loss: 2.0116283849347383e-05\n",
      "Epoch 815, Loss: 2.0056681023561396e-05\n",
      "Epoch 816, Loss: 2.0056679204571992e-05\n",
      "Epoch 817, Loss: 1.9967277694377117e-05\n",
      "Epoch 818, Loss: 1.993747537198942e-05\n",
      "Epoch 819, Loss: 1.993747537198942e-05\n",
      "Epoch 820, Loss: 1.984807022381574e-05\n",
      "Epoch 821, Loss: 1.9788467398029752e-05\n",
      "Epoch 822, Loss: 1.975866689463146e-05\n",
      "Epoch 823, Loss: 1.9669261746457778e-05\n",
      "Epoch 824, Loss: 1.9639459424070083e-05\n",
      "Epoch 825, Loss: 1.960965892067179e-05\n",
      "Epoch 826, Loss: 1.95500542758964e-05\n",
      "Epoch 827, Loss: 1.952025377249811e-05\n",
      "Epoch 828, Loss: 1.9430848624324426e-05\n",
      "Epoch 829, Loss: 1.940104630193673e-05\n",
      "Epoch 830, Loss: 1.9341443476150744e-05\n",
      "Epoch 831, Loss: 1.9281840650364757e-05\n",
      "Epoch 832, Loss: 1.925203832797706e-05\n",
      "Epoch 833, Loss: 1.922223782457877e-05\n",
      "Epoch 834, Loss: 1.9162634998792782e-05\n",
      "Epoch 835, Loss: 1.9103032173006795e-05\n",
      "Epoch 836, Loss: 1.9043429347220808e-05\n",
      "Epoch 837, Loss: 1.9043429347220808e-05\n",
      "Epoch 838, Loss: 1.9013627024833113e-05\n",
      "Epoch 839, Loss: 1.8924223695648834e-05\n",
      "Epoch 840, Loss: 1.889442137326114e-05\n",
      "Epoch 841, Loss: 1.8864619050873443e-05\n",
      "Epoch 842, Loss: 1.880501804407686e-05\n",
      "Epoch 843, Loss: 1.8775215721689165e-05\n",
      "Epoch 844, Loss: 1.874541339930147e-05\n",
      "Epoch 845, Loss: 1.8715611076913774e-05\n",
      "Epoch 846, Loss: 1.865601007011719e-05\n",
      "Epoch 847, Loss: 1.865601007011719e-05\n",
      "Epoch 848, Loss: 1.853680078056641e-05\n",
      "Epoch 849, Loss: 1.853680078056641e-05\n",
      "Epoch 850, Loss: 1.8477199773769826e-05\n",
      "Epoch 851, Loss: 1.841759694798384e-05\n",
      "Epoch 852, Loss: 1.8417595128994435e-05\n",
      "Epoch 853, Loss: 1.835799412219785e-05\n",
      "Epoch 854, Loss: 1.829838947742246e-05\n",
      "Epoch 855, Loss: 1.829838947742246e-05\n",
      "Epoch 856, Loss: 1.829838947742246e-05\n",
      "Epoch 857, Loss: 1.8179183825850487e-05\n",
      "Epoch 858, Loss: 1.814938150346279e-05\n",
      "Epoch 859, Loss: 1.81195810000645e-05\n",
      "Epoch 860, Loss: 1.8089780496666208e-05\n",
      "Epoch 861, Loss: 1.8000375348492526e-05\n",
      "Epoch 862, Loss: 1.797057302610483e-05\n",
      "Epoch 863, Loss: 1.7940770703717135e-05\n",
      "Epoch 864, Loss: 1.7881167877931148e-05\n",
      "Epoch 865, Loss: 1.782156505214516e-05\n",
      "Epoch 866, Loss: 1.782156505214516e-05\n",
      "Epoch 867, Loss: 1.7761962226359174e-05\n",
      "Epoch 868, Loss: 1.773215990397148e-05\n",
      "Epoch 869, Loss: 1.7702359400573187e-05\n",
      "Epoch 870, Loss: 1.767255707818549e-05\n",
      "Epoch 871, Loss: 1.758315193001181e-05\n",
      "Epoch 872, Loss: 1.758315193001181e-05\n",
      "Epoch 873, Loss: 1.7553349607624114e-05\n",
      "Epoch 874, Loss: 1.7523549104225822e-05\n",
      "Epoch 875, Loss: 1.749374860082753e-05\n",
      "Epoch 876, Loss: 1.743414395605214e-05\n",
      "Epoch 877, Loss: 1.7374541130266152e-05\n",
      "Epoch 878, Loss: 1.7314938304480165e-05\n",
      "Epoch 879, Loss: 1.728513598209247e-05\n",
      "Epoch 880, Loss: 1.728513598209247e-05\n",
      "Epoch 881, Loss: 1.728513598209247e-05\n",
      "Epoch 882, Loss: 1.7225533156306483e-05\n",
      "Epoch 883, Loss: 1.71361280081328e-05\n",
      "Epoch 884, Loss: 1.71361280081328e-05\n",
      "Epoch 885, Loss: 1.7106325685745105e-05\n",
      "Epoch 886, Loss: 1.7076525182346813e-05\n",
      "Epoch 887, Loss: 1.704672467894852e-05\n",
      "Epoch 888, Loss: 1.7016922356560826e-05\n",
      "Epoch 889, Loss: 1.6927517208387144e-05\n",
      "Epoch 890, Loss: 1.692751538939774e-05\n",
      "Epoch 891, Loss: 1.6867914382601157e-05\n",
      "Epoch 892, Loss: 1.683811206021346e-05\n",
      "Epoch 893, Loss: 1.6808309737825766e-05\n",
      "Epoch 894, Loss: 1.6808309737825766e-05\n",
      "Epoch 895, Loss: 1.674870691203978e-05\n",
      "Epoch 896, Loss: 1.6689104086253792e-05\n",
      "Epoch 897, Loss: 1.6659301763866097e-05\n",
      "Epoch 898, Loss: 1.6629501260467805e-05\n",
      "Epoch 899, Loss: 1.6569898434681818e-05\n",
      "Epoch 900, Loss: 1.6540096112294123e-05\n",
      "Epoch 901, Loss: 1.6510293789906427e-05\n",
      "Epoch 902, Loss: 1.6480493286508135e-05\n",
      "Epoch 903, Loss: 1.645069096412044e-05\n",
      "Epoch 904, Loss: 1.6361285815946758e-05\n",
      "Epoch 905, Loss: 1.6361285815946758e-05\n",
      "Epoch 906, Loss: 1.6361285815946758e-05\n",
      "Epoch 907, Loss: 1.6361285815946758e-05\n",
      "Epoch 908, Loss: 1.6242080164374784e-05\n",
      "Epoch 909, Loss: 1.6182475519599393e-05\n",
      "Epoch 910, Loss: 1.6182475519599393e-05\n",
      "Epoch 911, Loss: 1.6152673197211698e-05\n",
      "Epoch 912, Loss: 1.6122872693813406e-05\n",
      "Epoch 913, Loss: 1.6093072190415114e-05\n",
      "Epoch 914, Loss: 1.606326986802742e-05\n",
      "Epoch 915, Loss: 1.6033467545639724e-05\n",
      "Epoch 916, Loss: 1.6033467545639724e-05\n",
      "Epoch 917, Loss: 1.5973864719853736e-05\n",
      "Epoch 918, Loss: 1.591426189406775e-05\n",
      "Epoch 919, Loss: 1.591426189406775e-05\n",
      "Epoch 920, Loss: 1.591426189406775e-05\n",
      "Epoch 921, Loss: 1.5824856745894067e-05\n",
      "Epoch 922, Loss: 1.579505442350637e-05\n",
      "Epoch 923, Loss: 1.576525392010808e-05\n",
      "Epoch 924, Loss: 1.5735451597720385e-05\n",
      "Epoch 925, Loss: 1.570564927533269e-05\n",
      "Epoch 926, Loss: 1.5646046449546702e-05\n",
      "Epoch 927, Loss: 1.5646046449546702e-05\n",
      "Epoch 928, Loss: 1.5616244127159007e-05\n",
      "Epoch 929, Loss: 1.555664130137302e-05\n",
      "Epoch 930, Loss: 1.555664130137302e-05\n",
      "Epoch 931, Loss: 1.5526840797974728e-05\n",
      "Epoch 932, Loss: 1.5526840797974728e-05\n",
      "Epoch 933, Loss: 1.5467236153199337e-05\n",
      "Epoch 934, Loss: 1.540763332741335e-05\n",
      "Epoch 935, Loss: 1.534802868263796e-05\n",
      "Epoch 936, Loss: 1.534802868263796e-05\n",
      "Epoch 937, Loss: 1.534802868263796e-05\n",
      "Epoch 938, Loss: 1.5288425856851973e-05\n",
      "Epoch 939, Loss: 1.5288425856851973e-05\n",
      "Epoch 940, Loss: 1.5228823031065986e-05\n",
      "Epoch 941, Loss: 1.5228823031065986e-05\n",
      "Epoch 942, Loss: 1.5139417882892303e-05\n",
      "Epoch 943, Loss: 1.5109615560504608e-05\n",
      "Epoch 944, Loss: 1.5079814147611614e-05\n",
      "Epoch 945, Loss: 1.5020210412330925e-05\n",
      "Epoch 946, Loss: 1.5020211321825627e-05\n",
      "Epoch 947, Loss: 1.4990408999437932e-05\n",
      "Epoch 948, Loss: 1.4990409908932634e-05\n",
      "Epoch 949, Loss: 1.4960607586544938e-05\n",
      "Epoch 950, Loss: 1.4871202438371256e-05\n",
      "Epoch 951, Loss: 1.4871202438371256e-05\n",
      "Epoch 952, Loss: 1.484140011598356e-05\n",
      "Epoch 953, Loss: 1.484140011598356e-05\n",
      "Epoch 954, Loss: 1.4811599612585269e-05\n",
      "Epoch 955, Loss: 1.4781797290197574e-05\n",
      "Epoch 956, Loss: 1.4781797290197574e-05\n",
      "Epoch 957, Loss: 1.4722194464411587e-05\n",
      "Epoch 958, Loss: 1.4692392142023891e-05\n",
      "Epoch 959, Loss: 1.4632788406743202e-05\n",
      "Epoch 960, Loss: 1.4602986993850209e-05\n",
      "Epoch 961, Loss: 1.454338325856952e-05\n",
      "Epoch 962, Loss: 1.454338325856952e-05\n",
      "Epoch 963, Loss: 1.454338325856952e-05\n",
      "Epoch 964, Loss: 1.4483780432783533e-05\n",
      "Epoch 965, Loss: 1.445397901989054e-05\n",
      "Epoch 966, Loss: 1.4424176697502844e-05\n",
      "Epoch 967, Loss: 1.4394374375115149e-05\n",
      "Epoch 968, Loss: 1.4394374375115149e-05\n",
      "Epoch 969, Loss: 1.4394374375115149e-05\n",
      "Epoch 970, Loss: 1.4364573871716857e-05\n",
      "Epoch 971, Loss: 1.4304970136436168e-05\n",
      "Epoch 972, Loss: 1.424536640115548e-05\n",
      "Epoch 973, Loss: 1.424536640115548e-05\n",
      "Epoch 974, Loss: 1.424536640115548e-05\n",
      "Epoch 975, Loss: 1.4215564988262486e-05\n",
      "Epoch 976, Loss: 1.4185763575369492e-05\n",
      "Epoch 977, Loss: 1.4126158930594102e-05\n",
      "Epoch 978, Loss: 1.4126158930594102e-05\n",
      "Epoch 979, Loss: 1.4096356608206406e-05\n",
      "Epoch 980, Loss: 1.4066556104808114e-05\n",
      "Epoch 981, Loss: 1.4036754691915121e-05\n",
      "Epoch 982, Loss: 1.4006952369527426e-05\n",
      "Epoch 983, Loss: 1.3977150956634432e-05\n",
      "Epoch 984, Loss: 1.3977150956634432e-05\n",
      "Epoch 985, Loss: 1.3947349543741439e-05\n",
      "Epoch 986, Loss: 1.3887744898966048e-05\n",
      "Epoch 987, Loss: 1.3798340660287067e-05\n",
      "Epoch 988, Loss: 1.3798340660287067e-05\n",
      "Epoch 989, Loss: 1.3768538337899372e-05\n",
      "Epoch 990, Loss: 1.3768538337899372e-05\n",
      "Epoch 991, Loss: 1.373873783450108e-05\n",
      "Epoch 992, Loss: 1.3708935512113385e-05\n",
      "Epoch 993, Loss: 1.3708935512113385e-05\n",
      "Epoch 994, Loss: 1.3708935512113385e-05\n",
      "Epoch 995, Loss: 1.3649331776832696e-05\n",
      "Epoch 996, Loss: 1.3589728041552007e-05\n",
      "Epoch 997, Loss: 1.3589728041552007e-05\n",
      "Epoch 998, Loss: 1.3559926628659014e-05\n",
      "Epoch 999, Loss: 1.3559926628659014e-05\n",
      "Epoch 1000, Loss: 1.353012521576602e-05\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array([half_sphere_output, band_output, polar_caps_output, checkerboard_output])\n",
    "labels = [0,1,2,3]\n",
    "\n",
    "dataset = ShapeDataset(sequences, labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = Classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.3470521480485331e-05, Test Loss: 1.3440719158097636e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 2, Train Loss: 1.3440719158097636e-05, Test Loss: 1.3381115422816947e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 3, Train Loss: 1.3381115422816947e-05, Test Loss: 1.3381115422816947e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 4, Train Loss: 1.3381116332311649e-05, Test Loss: 1.3381115422816947e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 5, Train Loss: 1.3381115422816947e-05, Test Loss: 1.3351314009923954e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 6, Train Loss: 1.3351314009923954e-05, Test Loss: 1.3291710274643265e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 7, Train Loss: 1.3291710274643265e-05, Test Loss: 1.3291710274643265e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 8, Train Loss: 1.3291710274643265e-05, Test Loss: 1.326190795225557e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 9, Train Loss: 1.3261908861750271e-05, Test Loss: 1.3261908861750271e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 10, Train Loss: 1.326190795225557e-05, Test Loss: 1.3232107448857278e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 11, Train Loss: 1.3232106539362576e-05, Test Loss: 1.3232107448857278e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 12, Train Loss: 1.3232107448857278e-05, Test Loss: 1.3202305126469582e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 13, Train Loss: 1.3202305126469582e-05, Test Loss: 1.3142702300683595e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 14, Train Loss: 1.3142702300683595e-05, Test Loss: 1.3142701391188893e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 15, Train Loss: 1.3142702300683595e-05, Test Loss: 1.3083097655908205e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 16, Train Loss: 1.3083098565402906e-05, Test Loss: 1.3083097655908205e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 17, Train Loss: 1.3083097655908205e-05, Test Loss: 1.3053296243015211e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 18, Train Loss: 1.3053296243015211e-05, Test Loss: 1.3023494830122218e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 19, Train Loss: 1.3023494830122218e-05, Test Loss: 1.2963891094841529e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 20, Train Loss: 1.2963891094841529e-05, Test Loss: 1.2963891094841529e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 21, Train Loss: 1.2963891094841529e-05, Test Loss: 1.2963891094841529e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 22, Train Loss: 1.2963891094841529e-05, Test Loss: 1.2963891094841529e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 23, Train Loss: 1.2963891094841529e-05, Test Loss: 1.290428735956084e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 24, Train Loss: 1.290428735956084e-05, Test Loss: 1.290428735956084e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 25, Train Loss: 1.290428735956084e-05, Test Loss: 1.2874485037173145e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 26, Train Loss: 1.2874485037173145e-05, Test Loss: 1.2814882211387157e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 27, Train Loss: 1.2814882211387157e-05, Test Loss: 1.2785080798494164e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 28, Train Loss: 1.2785079888999462e-05, Test Loss: 1.2785079888999462e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 29, Train Loss: 1.2785079888999462e-05, Test Loss: 1.2755278476106469e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 30, Train Loss: 1.2755278476106469e-05, Test Loss: 1.2755278476106469e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 31, Train Loss: 1.2755278476106469e-05, Test Loss: 1.269567474082578e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 32, Train Loss: 1.269567474082578e-05, Test Loss: 1.269567474082578e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 33, Train Loss: 1.269567474082578e-05, Test Loss: 1.2665873327932786e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 34, Train Loss: 1.2665873327932786e-05, Test Loss: 1.2665873327932786e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 35, Train Loss: 1.2665873327932786e-05, Test Loss: 1.2606269592652097e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 36, Train Loss: 1.2606269592652097e-05, Test Loss: 1.2576467270264402e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 37, Train Loss: 1.2576467270264402e-05, Test Loss: 1.254666676686611e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 38, Train Loss: 1.254666676686611e-05, Test Loss: 1.2546665857371408e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 39, Train Loss: 1.254666676686611e-05, Test Loss: 1.2516864444478415e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 40, Train Loss: 1.2516864444478415e-05, Test Loss: 1.248706212209072e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 41, Train Loss: 1.248706212209072e-05, Test Loss: 1.2457260709197726e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 42, Train Loss: 1.2457261618692428e-05, Test Loss: 1.2457261618692428e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 43, Train Loss: 1.2457261618692428e-05, Test Loss: 1.2457260709197726e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 44, Train Loss: 1.2457260709197726e-05, Test Loss: 1.2397656973917037e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 45, Train Loss: 1.2397656973917037e-05, Test Loss: 1.2397656973917037e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 46, Train Loss: 1.2397656973917037e-05, Test Loss: 1.2397656973917037e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 47, Train Loss: 1.2397656973917037e-05, Test Loss: 1.2308251825743355e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 48, Train Loss: 1.2308251825743355e-05, Test Loss: 1.2248647180967964e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 49, Train Loss: 1.2248647180967964e-05, Test Loss: 1.2248647180967964e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 50, Train Loss: 1.2248647180967964e-05, Test Loss: 1.2218844858580269e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 51, Train Loss: 1.221884576807497e-05, Test Loss: 1.2218844858580269e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 52, Train Loss: 1.2218844858580269e-05, Test Loss: 1.2218844858580269e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 53, Train Loss: 1.2218844858580269e-05, Test Loss: 1.2159242942288984e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 54, Train Loss: 1.2159242942288984e-05, Test Loss: 1.2129439710406587e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 55, Train Loss: 1.2129440619901288e-05, Test Loss: 1.2129440619901288e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 56, Train Loss: 1.2129440619901288e-05, Test Loss: 1.2129440619901288e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 57, Train Loss: 1.2129440619901288e-05, Test Loss: 1.2099639207008295e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 58, Train Loss: 1.2099639207008295e-05, Test Loss: 1.2099639207008295e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 59, Train Loss: 1.2099639207008295e-05, Test Loss: 1.2099639207008295e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 60, Train Loss: 1.2099639207008295e-05, Test Loss: 1.2040035471727606e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 61, Train Loss: 1.2040034562232904e-05, Test Loss: 1.2010234058834612e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 62, Train Loss: 1.201023314933991e-05, Test Loss: 1.1980431736446917e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 63, Train Loss: 1.1980432645941619e-05, Test Loss: 1.1950630323553924e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 64, Train Loss: 1.1950630323553924e-05, Test Loss: 1.1950630323553924e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 65, Train Loss: 1.1950630323553924e-05, Test Loss: 1.1950630323553924e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 66, Train Loss: 1.1950630323553924e-05, Test Loss: 1.1891026588273235e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 67, Train Loss: 1.1891026588273235e-05, Test Loss: 1.1891025678778533e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 68, Train Loss: 1.1891026588273235e-05, Test Loss: 1.1891026588273235e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 69, Train Loss: 1.1891025678778533e-05, Test Loss: 1.186122426588554e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 70, Train Loss: 1.186122426588554e-05, Test Loss: 1.1831422852992546e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 71, Train Loss: 1.1831422852992546e-05, Test Loss: 1.180162053060485e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 72, Train Loss: 1.180162053060485e-05, Test Loss: 1.1742016795324162e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 73, Train Loss: 1.1742016795324162e-05, Test Loss: 1.1712215382431168e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 74, Train Loss: 1.1712215382431168e-05, Test Loss: 1.1712215382431168e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 75, Train Loss: 1.1712215382431168e-05, Test Loss: 1.1712215382431168e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 76, Train Loss: 1.1712215382431168e-05, Test Loss: 1.1682413969538175e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 77, Train Loss: 1.1682413969538175e-05, Test Loss: 1.165261164715048e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 78, Train Loss: 1.165261164715048e-05, Test Loss: 1.165261164715048e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 79, Train Loss: 1.165261164715048e-05, Test Loss: 1.1622809324762784e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 80, Train Loss: 1.1622809324762784e-05, Test Loss: 1.1622809324762784e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 81, Train Loss: 1.1622809324762784e-05, Test Loss: 1.159300791186979e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 82, Train Loss: 1.159300791186979e-05, Test Loss: 1.159300791186979e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 83, Train Loss: 1.159300791186979e-05, Test Loss: 1.159300791186979e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 84, Train Loss: 1.159300791186979e-05, Test Loss: 1.1533404176589102e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 85, Train Loss: 1.1533404176589102e-05, Test Loss: 1.1473799531813711e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 86, Train Loss: 1.1473800441308413e-05, Test Loss: 1.144399902841542e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 87, Train Loss: 1.144399902841542e-05, Test Loss: 1.144399902841542e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 88, Train Loss: 1.144399902841542e-05, Test Loss: 1.144399902841542e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 89, Train Loss: 1.144399902841542e-05, Test Loss: 1.144399902841542e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 90, Train Loss: 1.144399902841542e-05, Test Loss: 1.1414196706027724e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 91, Train Loss: 1.1414196706027724e-05, Test Loss: 1.1414196706027724e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 92, Train Loss: 1.1414196706027724e-05, Test Loss: 1.1354592970747035e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 93, Train Loss: 1.1354592061252333e-05, Test Loss: 1.1324791557854041e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 94, Train Loss: 1.1324791557854041e-05, Test Loss: 1.1324791557854041e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 95, Train Loss: 1.1324791557854041e-05, Test Loss: 1.1324791557854041e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 96, Train Loss: 1.1324791557854041e-05, Test Loss: 1.1324791557854041e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 97, Train Loss: 1.1324791557854041e-05, Test Loss: 1.1324791557854041e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 98, Train Loss: 1.1324791557854041e-05, Test Loss: 1.1294990144961048e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 99, Train Loss: 1.1294990144961048e-05, Test Loss: 1.1205583177797962e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 100, Train Loss: 1.1205584087292664e-05, Test Loss: 1.111617802962428e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 101, Train Loss: 1.1116178939118981e-05, Test Loss: 1.1116178939118981e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 102, Train Loss: 1.1116178939118981e-05, Test Loss: 1.1116178939118981e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 103, Train Loss: 1.1116178939118981e-05, Test Loss: 1.1116178939118981e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 104, Train Loss: 1.1116178939118981e-05, Test Loss: 1.111617802962428e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 105, Train Loss: 1.111617802962428e-05, Test Loss: 1.1086376616731286e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 106, Train Loss: 1.1086376616731286e-05, Test Loss: 1.1086376616731286e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 107, Train Loss: 1.1086376616731286e-05, Test Loss: 1.1026772881450597e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 108, Train Loss: 1.1026772881450597e-05, Test Loss: 1.1026773790945299e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 109, Train Loss: 1.1026773790945299e-05, Test Loss: 1.0996971468557604e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 110, Train Loss: 1.0996971468557604e-05, Test Loss: 1.0967169146169908e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 111, Train Loss: 1.0967169146169908e-05, Test Loss: 1.0967169146169908e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 112, Train Loss: 1.0967169146169908e-05, Test Loss: 1.0937366823782213e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 113, Train Loss: 1.0937366823782213e-05, Test Loss: 1.0937366823782213e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 114, Train Loss: 1.0937366823782213e-05, Test Loss: 1.0877763997996226e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 115, Train Loss: 1.0877763088501524e-05, Test Loss: 1.0877763088501524e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 116, Train Loss: 1.0877763088501524e-05, Test Loss: 1.0877763997996226e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 117, Train Loss: 1.0877763088501524e-05, Test Loss: 1.084796167560853e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 118, Train Loss: 1.084796167560853e-05, Test Loss: 1.084796167560853e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 119, Train Loss: 1.084796167560853e-05, Test Loss: 1.084796167560853e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 120, Train Loss: 1.084796167560853e-05, Test Loss: 1.084796167560853e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 121, Train Loss: 1.084796167560853e-05, Test Loss: 1.0788358849822544e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 122, Train Loss: 1.0788358849822544e-05, Test Loss: 1.0758556527434848e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 123, Train Loss: 1.0758556527434848e-05, Test Loss: 1.0758556527434848e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 124, Train Loss: 1.0758556527434848e-05, Test Loss: 1.0758556527434848e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 125, Train Loss: 1.0758556527434848e-05, Test Loss: 1.0758556527434848e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 126, Train Loss: 1.0758556527434848e-05, Test Loss: 1.0669150469766464e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 127, Train Loss: 1.0669149560271762e-05, Test Loss: 1.0669150469766464e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 128, Train Loss: 1.0669150469766464e-05, Test Loss: 1.0639348147378769e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 129, Train Loss: 1.063934905687347e-05, Test Loss: 1.0609546734485775e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 130, Train Loss: 1.0609546734485775e-05, Test Loss: 1.0609546734485775e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 131, Train Loss: 1.0609546734485775e-05, Test Loss: 1.0549942999205086e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 132, Train Loss: 1.0549942999205086e-05, Test Loss: 1.0549942999205086e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 133, Train Loss: 1.0549942999205086e-05, Test Loss: 1.0549942999205086e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 134, Train Loss: 1.0549942999205086e-05, Test Loss: 1.0549942999205086e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 135, Train Loss: 1.0549942999205086e-05, Test Loss: 1.0520140676817391e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 136, Train Loss: 1.0520140676817391e-05, Test Loss: 1.0490339263924398e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 137, Train Loss: 1.04903401734191e-05, Test Loss: 1.0490339263924398e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 138, Train Loss: 1.0490339263924398e-05, Test Loss: 1.04903401734191e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 139, Train Loss: 1.0490339263924398e-05, Test Loss: 1.0460536941536702e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 140, Train Loss: 1.0460537851031404e-05, Test Loss: 1.0460536941536702e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 141, Train Loss: 1.0460537851031404e-05, Test Loss: 1.0430735528643709e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 142, Train Loss: 1.0430735528643709e-05, Test Loss: 1.0400934115750715e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 143, Train Loss: 1.0400934115750715e-05, Test Loss: 1.0400933206256013e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 144, Train Loss: 1.0400934115750715e-05, Test Loss: 1.0341329470975325e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 145, Train Loss: 1.0341330380470026e-05, Test Loss: 1.0341330380470026e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 146, Train Loss: 1.0341330380470026e-05, Test Loss: 1.0341329470975325e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 147, Train Loss: 1.0341329470975325e-05, Test Loss: 1.0341329470975325e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 148, Train Loss: 1.0341329470975325e-05, Test Loss: 1.0281725735694636e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 149, Train Loss: 1.0281725735694636e-05, Test Loss: 1.0281725735694636e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 150, Train Loss: 1.0281725735694636e-05, Test Loss: 1.0281725735694636e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 151, Train Loss: 1.0281725735694636e-05, Test Loss: 1.0251924322801642e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 152, Train Loss: 1.0251924322801642e-05, Test Loss: 1.0251924322801642e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 153, Train Loss: 1.025192341330694e-05, Test Loss: 1.0192319678026251e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 154, Train Loss: 1.0192319678026251e-05, Test Loss: 1.0192319678026251e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 155, Train Loss: 1.0192319678026251e-05, Test Loss: 1.0192320587520953e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 156, Train Loss: 1.0192319678026251e-05, Test Loss: 1.0132716852240264e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 157, Train Loss: 1.0132716852240264e-05, Test Loss: 1.0132715942745563e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 158, Train Loss: 1.0132716852240264e-05, Test Loss: 1.0132716852240264e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 159, Train Loss: 1.0132716852240264e-05, Test Loss: 1.0073112207464874e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 160, Train Loss: 1.0073112207464874e-05, Test Loss: 1.0073112207464874e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 161, Train Loss: 1.0073112207464874e-05, Test Loss: 1.0073112207464874e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 162, Train Loss: 1.0073112207464874e-05, Test Loss: 1.0073112207464874e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 163, Train Loss: 1.0073112207464874e-05, Test Loss: 1.004331079457188e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 164, Train Loss: 1.004331079457188e-05, Test Loss: 1.0013509381678887e-05, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 165, Train Loss: 1.0013509381678887e-05, Test Loss: 9.983707059291191e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 166, Train Loss: 9.983707059291191e-06, Test Loss: 9.983707059291191e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 167, Train Loss: 9.983707059291191e-06, Test Loss: 9.953905646398198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 168, Train Loss: 9.953905646398198e-06, Test Loss: 9.953905646398198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 169, Train Loss: 9.953905646398198e-06, Test Loss: 9.924103324010503e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 170, Train Loss: 9.924103324010503e-06, Test Loss: 9.924103324010503e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 171, Train Loss: 9.924103324010503e-06, Test Loss: 9.924103324010503e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 172, Train Loss: 9.924103324010503e-06, Test Loss: 9.864499588729814e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 173, Train Loss: 9.864499588729814e-06, Test Loss: 9.834697266342118e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 174, Train Loss: 9.834697266342118e-06, Test Loss: 9.834697266342118e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 175, Train Loss: 9.834697266342118e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 176, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 177, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 178, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 179, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 180, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 181, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 182, Train Loss: 9.745292118168436e-06, Test Loss: 9.745292118168436e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 183, Train Loss: 9.745292118168436e-06, Test Loss: 9.71548979578074e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 184, Train Loss: 9.71548979578074e-06, Test Loss: 9.655886060500052e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 185, Train Loss: 9.655886060500052e-06, Test Loss: 9.655886060500052e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 186, Train Loss: 9.655886060500052e-06, Test Loss: 9.655886060500052e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 187, Train Loss: 9.655886060500052e-06, Test Loss: 9.626084647607058e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 188, Train Loss: 9.626084647607058e-06, Test Loss: 9.626084647607058e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 189, Train Loss: 9.626084647607058e-06, Test Loss: 9.596282325219363e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 190, Train Loss: 9.596282325219363e-06, Test Loss: 9.596282325219363e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 191, Train Loss: 9.596282325219363e-06, Test Loss: 9.596282325219363e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 192, Train Loss: 9.596282325219363e-06, Test Loss: 9.566480002831668e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 193, Train Loss: 9.566480002831668e-06, Test Loss: 9.50687717704568e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 194, Train Loss: 9.50687717704568e-06, Test Loss: 9.477074854657985e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 195, Train Loss: 9.477074854657985e-06, Test Loss: 9.477074854657985e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 196, Train Loss: 9.477074854657985e-06, Test Loss: 9.477074854657985e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 197, Train Loss: 9.477074854657985e-06, Test Loss: 9.44727253227029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 198, Train Loss: 9.44727253227029e-06, Test Loss: 9.44727253227029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 199, Train Loss: 9.44727253227029e-06, Test Loss: 9.417471119377296e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 200, Train Loss: 9.417470209882595e-06, Test Loss: 9.417470209882595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 201, Train Loss: 9.417470209882595e-06, Test Loss: 9.387668796989601e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 202, Train Loss: 9.387668796989601e-06, Test Loss: 9.387668796989601e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 203, Train Loss: 9.387668796989601e-06, Test Loss: 9.387668796989601e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 204, Train Loss: 9.387668796989601e-06, Test Loss: 9.328065061708912e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 205, Train Loss: 9.328065061708912e-06, Test Loss: 9.328065061708912e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 206, Train Loss: 9.328065061708912e-06, Test Loss: 9.328065061708912e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 207, Train Loss: 9.328065061708912e-06, Test Loss: 9.298262739321217e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 208, Train Loss: 9.298262739321217e-06, Test Loss: 9.298262739321217e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 209, Train Loss: 9.298262739321217e-06, Test Loss: 9.298262739321217e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 210, Train Loss: 9.298262739321217e-06, Test Loss: 9.268461326428223e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 211, Train Loss: 9.268461326428223e-06, Test Loss: 9.238659004040528e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 212, Train Loss: 9.23865991353523e-06, Test Loss: 9.23865991353523e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 213, Train Loss: 9.238659004040528e-06, Test Loss: 9.238659004040528e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 214, Train Loss: 9.23865991353523e-06, Test Loss: 9.208857591147535e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 215, Train Loss: 9.208857591147535e-06, Test Loss: 9.17905526875984e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 216, Train Loss: 9.17905526875984e-06, Test Loss: 9.17905526875984e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 217, Train Loss: 9.17905526875984e-06, Test Loss: 9.17905526875984e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 218, Train Loss: 9.17905526875984e-06, Test Loss: 9.17905526875984e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 219, Train Loss: 9.17905526875984e-06, Test Loss: 9.149252946372144e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 220, Train Loss: 9.149252946372144e-06, Test Loss: 9.149252946372144e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 221, Train Loss: 9.149252946372144e-06, Test Loss: 9.149252946372144e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 222, Train Loss: 9.149252946372144e-06, Test Loss: 9.149252946372144e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 223, Train Loss: 9.149252946372144e-06, Test Loss: 9.149252946372144e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 224, Train Loss: 9.149252946372144e-06, Test Loss: 9.089650120586157e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 225, Train Loss: 9.089650120586157e-06, Test Loss: 9.059847798198462e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 226, Train Loss: 9.059847798198462e-06, Test Loss: 9.000243153423071e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 227, Train Loss: 9.000243153423071e-06, Test Loss: 9.000243153423071e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 228, Train Loss: 9.000243153423071e-06, Test Loss: 8.970441740530077e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 229, Train Loss: 8.970441740530077e-06, Test Loss: 8.970441740530077e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 230, Train Loss: 8.970441740530077e-06, Test Loss: 8.940639418142382e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 231, Train Loss: 8.940639418142382e-06, Test Loss: 8.940639418142382e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 232, Train Loss: 8.940639418142382e-06, Test Loss: 8.940639418142382e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 233, Train Loss: 8.940639418142382e-06, Test Loss: 8.940639418142382e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 234, Train Loss: 8.940639418142382e-06, Test Loss: 8.940639418142382e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 235, Train Loss: 8.940639418142382e-06, Test Loss: 8.910838005249389e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 236, Train Loss: 8.910837095754687e-06, Test Loss: 8.910837095754687e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 237, Train Loss: 8.910838005249389e-06, Test Loss: 8.851233360473998e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 238, Train Loss: 8.8512342699687e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 239, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 240, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 241, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 242, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 243, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 244, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 245, Train Loss: 8.821431947581004e-06, Test Loss: 8.821431947581004e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 246, Train Loss: 8.821431947581004e-06, Test Loss: 8.73202588991262e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 247, Train Loss: 8.73202588991262e-06, Test Loss: 8.73202588991262e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 248, Train Loss: 8.73202588991262e-06, Test Loss: 8.73202588991262e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 249, Train Loss: 8.73202588991262e-06, Test Loss: 8.702223567524925e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 250, Train Loss: 8.702223567524925e-06, Test Loss: 8.672422154631931e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 251, Train Loss: 8.672422154631931e-06, Test Loss: 8.672422154631931e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 252, Train Loss: 8.672422154631931e-06, Test Loss: 8.642620741738938e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 253, Train Loss: 8.642620741738938e-06, Test Loss: 8.642620741738938e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 254, Train Loss: 8.642620741738938e-06, Test Loss: 8.612818419351242e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 255, Train Loss: 8.612818419351242e-06, Test Loss: 8.612818419351242e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 256, Train Loss: 8.612818419351242e-06, Test Loss: 8.583016096963547e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 257, Train Loss: 8.583016096963547e-06, Test Loss: 8.583016096963547e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 258, Train Loss: 8.583016096963547e-06, Test Loss: 8.553213774575852e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 259, Train Loss: 8.553214684070554e-06, Test Loss: 8.553214684070554e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 260, Train Loss: 8.553214684070554e-06, Test Loss: 8.553214684070554e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 261, Train Loss: 8.553214684070554e-06, Test Loss: 8.553214684070554e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 262, Train Loss: 8.553214684070554e-06, Test Loss: 8.523412361682858e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 263, Train Loss: 8.523412361682858e-06, Test Loss: 8.493610948789865e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 264, Train Loss: 8.493610948789865e-06, Test Loss: 8.493610948789865e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 265, Train Loss: 8.493610948789865e-06, Test Loss: 8.493610948789865e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 266, Train Loss: 8.493610948789865e-06, Test Loss: 8.434006304014474e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 267, Train Loss: 8.434006304014474e-06, Test Loss: 8.434006304014474e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 268, Train Loss: 8.434006304014474e-06, Test Loss: 8.434006304014474e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 269, Train Loss: 8.434006304014474e-06, Test Loss: 8.434006304014474e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 270, Train Loss: 8.434006304014474e-06, Test Loss: 8.374401659239084e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 271, Train Loss: 8.374402568733785e-06, Test Loss: 8.374401659239084e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 272, Train Loss: 8.374402568733785e-06, Test Loss: 8.374401659239084e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 273, Train Loss: 8.374401659239084e-06, Test Loss: 8.374402568733785e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 274, Train Loss: 8.374402568733785e-06, Test Loss: 8.344601155840792e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 275, Train Loss: 8.344601155840792e-06, Test Loss: 8.344601155840792e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 276, Train Loss: 8.344601155840792e-06, Test Loss: 8.314798833453096e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 277, Train Loss: 8.314798833453096e-06, Test Loss: 8.255194188677706e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 278, Train Loss: 8.255194188677706e-06, Test Loss: 8.255194188677706e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 279, Train Loss: 8.255194188677706e-06, Test Loss: 8.255194188677706e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 280, Train Loss: 8.255194188677706e-06, Test Loss: 8.255194188677706e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 281, Train Loss: 8.255194188677706e-06, Test Loss: 8.255194188677706e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 282, Train Loss: 8.255194188677706e-06, Test Loss: 8.22539186629001e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 283, Train Loss: 8.22539186629001e-06, Test Loss: 8.22539186629001e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 284, Train Loss: 8.22539186629001e-06, Test Loss: 8.22539186629001e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 285, Train Loss: 8.22539186629001e-06, Test Loss: 8.195589543902315e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 286, Train Loss: 8.195590453397017e-06, Test Loss: 8.165788131009322e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 287, Train Loss: 8.165788131009322e-06, Test Loss: 8.135985808621626e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 288, Train Loss: 8.135986718116328e-06, Test Loss: 8.106184395728633e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 289, Train Loss: 8.106184395728633e-06, Test Loss: 8.106184395728633e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 290, Train Loss: 8.106184395728633e-06, Test Loss: 8.106184395728633e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 291, Train Loss: 8.106184395728633e-06, Test Loss: 8.106184395728633e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 292, Train Loss: 8.106184395728633e-06, Test Loss: 8.076382073340937e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 293, Train Loss: 8.076382073340937e-06, Test Loss: 8.076382073340937e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 294, Train Loss: 8.076382073340937e-06, Test Loss: 8.046579750953242e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 295, Train Loss: 8.046580660447944e-06, Test Loss: 8.046580660447944e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 296, Train Loss: 8.046580660447944e-06, Test Loss: 8.046579750953242e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 297, Train Loss: 8.046580660447944e-06, Test Loss: 8.016778338060249e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 298, Train Loss: 8.016778338060249e-06, Test Loss: 8.016778338060249e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 299, Train Loss: 8.016778338060249e-06, Test Loss: 8.016778338060249e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 300, Train Loss: 8.016778338060249e-06, Test Loss: 7.986976015672553e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 301, Train Loss: 7.986976015672553e-06, Test Loss: 7.986976925167255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 302, Train Loss: 7.986976925167255e-06, Test Loss: 7.986976925167255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 303, Train Loss: 7.986976925167255e-06, Test Loss: 7.986976925167255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 304, Train Loss: 7.986976925167255e-06, Test Loss: 7.986976925167255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 305, Train Loss: 7.986976925167255e-06, Test Loss: 7.986976925167255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 306, Train Loss: 7.986976925167255e-06, Test Loss: 7.95717460277956e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 307, Train Loss: 7.95717460277956e-06, Test Loss: 7.897570867498871e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 308, Train Loss: 7.897570867498871e-06, Test Loss: 7.897570867498871e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 309, Train Loss: 7.897570867498871e-06, Test Loss: 7.897570867498871e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 310, Train Loss: 7.897570867498871e-06, Test Loss: 7.897570867498871e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 311, Train Loss: 7.897570867498871e-06, Test Loss: 7.867768545111176e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 312, Train Loss: 7.867768545111176e-06, Test Loss: 7.867768545111176e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 313, Train Loss: 7.867768545111176e-06, Test Loss: 7.867768545111176e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 314, Train Loss: 7.867768545111176e-06, Test Loss: 7.808164809830487e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 315, Train Loss: 7.808164809830487e-06, Test Loss: 7.808164809830487e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 316, Train Loss: 7.808164809830487e-06, Test Loss: 7.808164809830487e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 317, Train Loss: 7.808164809830487e-06, Test Loss: 7.808164809830487e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 318, Train Loss: 7.808164809830487e-06, Test Loss: 7.778362487442791e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 319, Train Loss: 7.778362487442791e-06, Test Loss: 7.748560165055096e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 320, Train Loss: 7.748560165055096e-06, Test Loss: 7.748560165055096e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 321, Train Loss: 7.748560165055096e-06, Test Loss: 7.718758752162103e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 322, Train Loss: 7.718758752162103e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 323, Train Loss: 7.688956429774407e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 324, Train Loss: 7.688956429774407e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 325, Train Loss: 7.688956429774407e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 326, Train Loss: 7.688956429774407e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 327, Train Loss: 7.688956429774407e-06, Test Loss: 7.688956429774407e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 328, Train Loss: 7.688956429774407e-06, Test Loss: 7.659155016881414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 329, Train Loss: 7.659155016881414e-06, Test Loss: 7.659155016881414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 330, Train Loss: 7.659154107386712e-06, Test Loss: 7.6293522397463676e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 331, Train Loss: 7.6293522397463676e-06, Test Loss: 7.6293526944937184e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 332, Train Loss: 7.6293526944937184e-06, Test Loss: 7.6293522397463676e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 333, Train Loss: 7.6293522397463676e-06, Test Loss: 7.6293526944937184e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 334, Train Loss: 7.6293526944937184e-06, Test Loss: 7.6293522397463676e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 335, Train Loss: 7.6293522397463676e-06, Test Loss: 7.569748504465679e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 336, Train Loss: 7.569748504465679e-06, Test Loss: 7.569748504465679e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 337, Train Loss: 7.569748504465679e-06, Test Loss: 7.539946636825334e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 338, Train Loss: 7.539946182077983e-06, Test Loss: 7.539946182077983e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 339, Train Loss: 7.539946182077983e-06, Test Loss: 7.539946182077983e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 340, Train Loss: 7.539946636825334e-06, Test Loss: 7.539946636825334e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 341, Train Loss: 7.539946182077983e-06, Test Loss: 7.510144314437639e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 342, Train Loss: 7.51014476918499e-06, Test Loss: 7.510144314437639e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 343, Train Loss: 7.51014476918499e-06, Test Loss: 7.4803429015446454e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 344, Train Loss: 7.4803429015446454e-06, Test Loss: 7.4803429015446454e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 345, Train Loss: 7.4803429015446454e-06, Test Loss: 7.4803424467972945e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 346, Train Loss: 7.4803429015446454e-06, Test Loss: 7.4803429015446454e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 347, Train Loss: 7.4803424467972945e-06, Test Loss: 7.4803429015446454e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 348, Train Loss: 7.4803424467972945e-06, Test Loss: 7.4803429015446454e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 349, Train Loss: 7.4803424467972945e-06, Test Loss: 7.45054057915695e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 350, Train Loss: 7.45054057915695e-06, Test Loss: 7.45054057915695e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 351, Train Loss: 7.45054057915695e-06, Test Loss: 7.420738256769255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 352, Train Loss: 7.420738711516606e-06, Test Loss: 7.420738256769255e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 353, Train Loss: 7.420738711516606e-06, Test Loss: 7.361134521488566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 354, Train Loss: 7.361134521488566e-06, Test Loss: 7.361134521488566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 355, Train Loss: 7.361134521488566e-06, Test Loss: 7.361134521488566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 356, Train Loss: 7.361134521488566e-06, Test Loss: 7.3313326538482215e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 357, Train Loss: 7.331333108595572e-06, Test Loss: 7.331333108595572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 358, Train Loss: 7.3313326538482215e-06, Test Loss: 7.301530786207877e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 359, Train Loss: 7.301530786207877e-06, Test Loss: 7.301530786207877e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 360, Train Loss: 7.301530786207877e-06, Test Loss: 7.271728463820182e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 361, Train Loss: 7.271728463820182e-06, Test Loss: 7.271728463820182e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 362, Train Loss: 7.271728463820182e-06, Test Loss: 7.271728463820182e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 363, Train Loss: 7.271728463820182e-06, Test Loss: 7.2419261414324865e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 364, Train Loss: 7.2419261414324865e-06, Test Loss: 7.212124273792142e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 365, Train Loss: 7.212124273792142e-06, Test Loss: 7.182322406151798e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 366, Train Loss: 7.182322406151798e-06, Test Loss: 7.182322406151798e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 367, Train Loss: 7.182322406151798e-06, Test Loss: 7.182322406151798e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 368, Train Loss: 7.182322406151798e-06, Test Loss: 7.182322406151798e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 369, Train Loss: 7.182322406151798e-06, Test Loss: 7.152520083764102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 370, Train Loss: 7.152520083764102e-06, Test Loss: 7.152520083764102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 371, Train Loss: 7.152520083764102e-06, Test Loss: 7.152520083764102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 372, Train Loss: 7.152520083764102e-06, Test Loss: 7.152520083764102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 373, Train Loss: 7.152520538511453e-06, Test Loss: 7.152520083764102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 374, Train Loss: 7.152520083764102e-06, Test Loss: 7.122718216123758e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 375, Train Loss: 7.122718216123758e-06, Test Loss: 7.122718216123758e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 376, Train Loss: 7.122718216123758e-06, Test Loss: 7.122718216123758e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 377, Train Loss: 7.122718216123758e-06, Test Loss: 7.0929163484834135e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 378, Train Loss: 7.0929163484834135e-06, Test Loss: 7.0929163484834135e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 379, Train Loss: 7.0929163484834135e-06, Test Loss: 7.063114026095718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 380, Train Loss: 7.063114026095718e-06, Test Loss: 7.063114026095718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 381, Train Loss: 7.063114026095718e-06, Test Loss: 7.063114026095718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 382, Train Loss: 7.063114480843069e-06, Test Loss: 7.063114026095718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 383, Train Loss: 7.063114026095718e-06, Test Loss: 7.063114480843069e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 384, Train Loss: 7.063114026095718e-06, Test Loss: 7.063114480843069e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 385, Train Loss: 7.063114026095718e-06, Test Loss: 7.063114026095718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 386, Train Loss: 7.063114026095718e-06, Test Loss: 7.033312613202725e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 387, Train Loss: 7.033312613202725e-06, Test Loss: 7.033312613202725e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 388, Train Loss: 7.033312613202725e-06, Test Loss: 7.003510290815029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 389, Train Loss: 7.003510290815029e-06, Test Loss: 6.973707968427334e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 390, Train Loss: 6.973707968427334e-06, Test Loss: 6.9439065555343404e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 391, Train Loss: 6.9439065555343404e-06, Test Loss: 6.9439065555343404e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 392, Train Loss: 6.9439065555343404e-06, Test Loss: 6.9439065555343404e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 393, Train Loss: 6.9439065555343404e-06, Test Loss: 6.9439065555343404e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 394, Train Loss: 6.9439065555343404e-06, Test Loss: 6.9439065555343404e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 395, Train Loss: 6.9439065555343404e-06, Test Loss: 6.914104233146645e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 396, Train Loss: 6.914104233146645e-06, Test Loss: 6.88430191075895e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 397, Train Loss: 6.88430191075895e-06, Test Loss: 6.884302365506301e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 398, Train Loss: 6.88430191075895e-06, Test Loss: 6.88430191075895e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 399, Train Loss: 6.88430191075895e-06, Test Loss: 6.884302365506301e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 400, Train Loss: 6.884302365506301e-06, Test Loss: 6.88430191075895e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 401, Train Loss: 6.884302365506301e-06, Test Loss: 6.884302365506301e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 402, Train Loss: 6.884302365506301e-06, Test Loss: 6.854500043118605e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 403, Train Loss: 6.854500043118605e-06, Test Loss: 6.854500497865956e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 404, Train Loss: 6.854500043118605e-06, Test Loss: 6.824698175478261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 405, Train Loss: 6.824698175478261e-06, Test Loss: 6.824698175478261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 406, Train Loss: 6.824698175478261e-06, Test Loss: 6.794895853090566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 407, Train Loss: 6.7948963078379165e-06, Test Loss: 6.794895853090566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 408, Train Loss: 6.794895853090566e-06, Test Loss: 6.794895853090566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 409, Train Loss: 6.794895853090566e-06, Test Loss: 6.794895853090566e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 410, Train Loss: 6.7948963078379165e-06, Test Loss: 6.7948963078379165e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 411, Train Loss: 6.794895853090566e-06, Test Loss: 6.765093985450221e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 412, Train Loss: 6.765093985450221e-06, Test Loss: 6.765093985450221e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 413, Train Loss: 6.765093985450221e-06, Test Loss: 6.765093985450221e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 414, Train Loss: 6.765093985450221e-06, Test Loss: 6.765093985450221e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 415, Train Loss: 6.765093985450221e-06, Test Loss: 6.735292117809877e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 416, Train Loss: 6.735292117809877e-06, Test Loss: 6.735292117809877e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 417, Train Loss: 6.735292117809877e-06, Test Loss: 6.675687927781837e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 418, Train Loss: 6.675687927781837e-06, Test Loss: 6.675687927781837e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 419, Train Loss: 6.675687927781837e-06, Test Loss: 6.675687927781837e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 420, Train Loss: 6.675687927781837e-06, Test Loss: 6.675687927781837e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 421, Train Loss: 6.675687927781837e-06, Test Loss: 6.675687927781837e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 422, Train Loss: 6.675687927781837e-06, Test Loss: 6.645886060141493e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 423, Train Loss: 6.645886060141493e-06, Test Loss: 6.645886060141493e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 424, Train Loss: 6.645886060141493e-06, Test Loss: 6.616083737753797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 425, Train Loss: 6.616083737753797e-06, Test Loss: 6.616083737753797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 426, Train Loss: 6.616083737753797e-06, Test Loss: 6.616083737753797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 427, Train Loss: 6.616083737753797e-06, Test Loss: 6.616083737753797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 428, Train Loss: 6.616083737753797e-06, Test Loss: 6.616083737753797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 429, Train Loss: 6.616083737753797e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 430, Train Loss: 6.5564800024731085e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 431, Train Loss: 6.5564800024731085e-06, Test Loss: 6.586282324860804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 432, Train Loss: 6.586282324860804e-06, Test Loss: 6.586282324860804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 433, Train Loss: 6.586282324860804e-06, Test Loss: 6.586282324860804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 434, Train Loss: 6.586282324860804e-06, Test Loss: 6.586282324860804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 435, Train Loss: 6.586282324860804e-06, Test Loss: 6.586282324860804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 436, Train Loss: 6.586282324860804e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 437, Train Loss: 6.5564800024731085e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 438, Train Loss: 6.5564800024731085e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 439, Train Loss: 6.5564800024731085e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 440, Train Loss: 6.5564800024731085e-06, Test Loss: 6.5564800024731085e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 441, Train Loss: 6.5564800024731085e-06, Test Loss: 6.496875357697718e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 442, Train Loss: 6.496875357697718e-06, Test Loss: 6.467073490057373e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 443, Train Loss: 6.467073490057373e-06, Test Loss: 6.437271622417029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 444, Train Loss: 6.437271622417029e-06, Test Loss: 6.437271622417029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 445, Train Loss: 6.437271622417029e-06, Test Loss: 6.437271622417029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 446, Train Loss: 6.437271622417029e-06, Test Loss: 6.437271622417029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 447, Train Loss: 6.437271622417029e-06, Test Loss: 6.407469300029334e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 448, Train Loss: 6.407469300029334e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 449, Train Loss: 6.377666977641638e-06, Test Loss: 6.377667432388989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 450, Train Loss: 6.377666977641638e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 451, Train Loss: 6.377666977641638e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 452, Train Loss: 6.377667432388989e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 453, Train Loss: 6.377667432388989e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 454, Train Loss: 6.377666977641638e-06, Test Loss: 6.377667432388989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 455, Train Loss: 6.377666977641638e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 456, Train Loss: 6.377666977641638e-06, Test Loss: 6.377666977641638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 457, Train Loss: 6.377666977641638e-06, Test Loss: 6.347865564748645e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 458, Train Loss: 6.347865564748645e-06, Test Loss: 6.347865564748645e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 459, Train Loss: 6.347865564748645e-06, Test Loss: 6.347865564748645e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 460, Train Loss: 6.347865564748645e-06, Test Loss: 6.288260919973254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 461, Train Loss: 6.288260919973254e-06, Test Loss: 6.288261374720605e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 462, Train Loss: 6.288261374720605e-06, Test Loss: 6.288260919973254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 463, Train Loss: 6.288261374720605e-06, Test Loss: 6.288260919973254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 464, Train Loss: 6.288261374720605e-06, Test Loss: 6.288261374720605e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 465, Train Loss: 6.288261374720605e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 466, Train Loss: 6.258459507080261e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 467, Train Loss: 6.258459507080261e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 468, Train Loss: 6.258459507080261e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 469, Train Loss: 6.258459507080261e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 470, Train Loss: 6.258459507080261e-06, Test Loss: 6.258459507080261e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 471, Train Loss: 6.258459507080261e-06, Test Loss: 6.228657184692565e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 472, Train Loss: 6.228657184692565e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 473, Train Loss: 6.198855771799572e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 474, Train Loss: 6.198855771799572e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 475, Train Loss: 6.198855771799572e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 476, Train Loss: 6.198855771799572e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 477, Train Loss: 6.198855771799572e-06, Test Loss: 6.198855771799572e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 478, Train Loss: 6.198855771799572e-06, Test Loss: 6.1690534494118765e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 479, Train Loss: 6.1690534494118765e-06, Test Loss: 6.1690534494118765e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 480, Train Loss: 6.1690534494118765e-06, Test Loss: 6.109448804636486e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 481, Train Loss: 6.109448804636486e-06, Test Loss: 6.0796469369961414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 482, Train Loss: 6.0796469369961414e-06, Test Loss: 6.0796469369961414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 483, Train Loss: 6.0796469369961414e-06, Test Loss: 6.0796469369961414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 484, Train Loss: 6.0796464822487906e-06, Test Loss: 6.0796469369961414e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 485, Train Loss: 6.0796469369961414e-06, Test Loss: 6.049845069355797e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 486, Train Loss: 6.049845069355797e-06, Test Loss: 6.020042746968102e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 487, Train Loss: 6.020043201715453e-06, Test Loss: 6.020043201715453e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 488, Train Loss: 6.020043201715453e-06, Test Loss: 6.020043201715453e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 489, Train Loss: 6.020043201715453e-06, Test Loss: 6.020043201715453e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 490, Train Loss: 6.020043201715453e-06, Test Loss: 5.990241334075108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 491, Train Loss: 5.990240879327757e-06, Test Loss: 5.990241334075108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 492, Train Loss: 5.990241334075108e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 493, Train Loss: 5.990240879327757e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 494, Train Loss: 5.990241334075108e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 495, Train Loss: 5.990240879327757e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 496, Train Loss: 5.990241334075108e-06, Test Loss: 5.990241334075108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 497, Train Loss: 5.990240879327757e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 498, Train Loss: 5.990240879327757e-06, Test Loss: 5.990241334075108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 499, Train Loss: 5.990240879327757e-06, Test Loss: 5.990240879327757e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 500, Train Loss: 5.990240879327757e-06, Test Loss: 5.9306366892997175e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 501, Train Loss: 5.9306366892997175e-06, Test Loss: 5.900835276406724e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 502, Train Loss: 5.900835276406724e-06, Test Loss: 5.900835276406724e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 503, Train Loss: 5.900835276406724e-06, Test Loss: 5.900835276406724e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 504, Train Loss: 5.900834821659373e-06, Test Loss: 5.900835276406724e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 505, Train Loss: 5.900834821659373e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 506, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 507, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 508, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 509, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 510, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 511, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 512, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 513, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 514, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 515, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 516, Train Loss: 5.871032954019029e-06, Test Loss: 5.871032954019029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 517, Train Loss: 5.871032954019029e-06, Test Loss: 5.811428309243638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 518, Train Loss: 5.811428309243638e-06, Test Loss: 5.811428309243638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 519, Train Loss: 5.811428309243638e-06, Test Loss: 5.811428309243638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 520, Train Loss: 5.811428309243638e-06, Test Loss: 5.811428309243638e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 521, Train Loss: 5.811428309243638e-06, Test Loss: 5.781626441603294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 522, Train Loss: 5.781626441603294e-06, Test Loss: 5.751824119215598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 523, Train Loss: 5.751824573962949e-06, Test Loss: 5.751824119215598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 524, Train Loss: 5.751824573962949e-06, Test Loss: 5.751824573962949e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 525, Train Loss: 5.751824573962949e-06, Test Loss: 5.751824573962949e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 526, Train Loss: 5.751824573962949e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 527, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 528, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 529, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 530, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 531, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 532, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 533, Train Loss: 5.722022251575254e-06, Test Loss: 5.722022251575254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 534, Train Loss: 5.722022251575254e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 535, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 536, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 537, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 538, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 539, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 540, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 541, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 542, Train Loss: 5.6922203839349095e-06, Test Loss: 5.6922203839349095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 543, Train Loss: 5.6922203839349095e-06, Test Loss: 5.662418061547214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 544, Train Loss: 5.662418516294565e-06, Test Loss: 5.63261619390687e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 545, Train Loss: 5.63261619390687e-06, Test Loss: 5.6028138715191744e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 546, Train Loss: 5.6028138715191744e-06, Test Loss: 5.6028138715191744e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 547, Train Loss: 5.6028138715191744e-06, Test Loss: 5.573011549131479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 548, Train Loss: 5.573011549131479e-06, Test Loss: 5.543209681491135e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 549, Train Loss: 5.543209681491135e-06, Test Loss: 5.543209681491135e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 550, Train Loss: 5.543209681491135e-06, Test Loss: 5.513407359103439e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 551, Train Loss: 5.513407359103439e-06, Test Loss: 5.513407359103439e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 552, Train Loss: 5.513407359103439e-06, Test Loss: 5.513407359103439e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 553, Train Loss: 5.513407359103439e-06, Test Loss: 5.513407359103439e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 554, Train Loss: 5.513407359103439e-06, Test Loss: 5.513407359103439e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 555, Train Loss: 5.513407359103439e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 556, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 557, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 558, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 559, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 560, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 561, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 562, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 563, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 564, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 565, Train Loss: 5.483605491463095e-06, Test Loss: 5.483605491463095e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 566, Train Loss: 5.483605491463095e-06, Test Loss: 5.424000846687704e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 567, Train Loss: 5.424000846687704e-06, Test Loss: 5.394199433794711e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 568, Train Loss: 5.394199433794711e-06, Test Loss: 5.394199433794711e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 569, Train Loss: 5.394199433794711e-06, Test Loss: 5.394199433794711e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 570, Train Loss: 5.394199433794711e-06, Test Loss: 5.394199433794711e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 571, Train Loss: 5.394199433794711e-06, Test Loss: 5.394199433794711e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 572, Train Loss: 5.394199433794711e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 573, Train Loss: 5.364397566154366e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 574, Train Loss: 5.3643971114070155e-06, Test Loss: 5.364397566154366e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 575, Train Loss: 5.3643971114070155e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 576, Train Loss: 5.3643971114070155e-06, Test Loss: 5.364397566154366e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 577, Train Loss: 5.3643971114070155e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 578, Train Loss: 5.3643971114070155e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 579, Train Loss: 5.3643971114070155e-06, Test Loss: 5.364397566154366e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 580, Train Loss: 5.3643971114070155e-06, Test Loss: 5.364397566154366e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 581, Train Loss: 5.364397566154366e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 582, Train Loss: 5.364397566154366e-06, Test Loss: 5.364397566154366e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 583, Train Loss: 5.3643971114070155e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 584, Train Loss: 5.3643971114070155e-06, Test Loss: 5.3643971114070155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 585, Train Loss: 5.3643971114070155e-06, Test Loss: 5.334595243766671e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 586, Train Loss: 5.334595243766671e-06, Test Loss: 5.334595243766671e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 587, Train Loss: 5.334595243766671e-06, Test Loss: 5.334595243766671e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 588, Train Loss: 5.334595243766671e-06, Test Loss: 5.334595243766671e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 589, Train Loss: 5.334595243766671e-06, Test Loss: 5.334595243766671e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 590, Train Loss: 5.334595243766671e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 591, Train Loss: 5.274991053738631e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 592, Train Loss: 5.274991053738631e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 593, Train Loss: 5.274991053738631e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 594, Train Loss: 5.274991053738631e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 595, Train Loss: 5.274991053738631e-06, Test Loss: 5.274991053738631e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 596, Train Loss: 5.274991053738631e-06, Test Loss: 5.245188731350936e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 597, Train Loss: 5.245189186098287e-06, Test Loss: 5.245189186098287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 598, Train Loss: 5.245189186098287e-06, Test Loss: 5.215386408963241e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 599, Train Loss: 5.2153868637105916e-06, Test Loss: 5.2153868637105916e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 600, Train Loss: 5.215386408963241e-06, Test Loss: 5.215386408963241e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 601, Train Loss: 5.215386408963241e-06, Test Loss: 5.2153868637105916e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 602, Train Loss: 5.215386408963241e-06, Test Loss: 5.2153868637105916e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 603, Train Loss: 5.215386408963241e-06, Test Loss: 5.185584541322896e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 604, Train Loss: 5.185584541322896e-06, Test Loss: 5.185584541322896e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 605, Train Loss: 5.185584541322896e-06, Test Loss: 5.185584541322896e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 606, Train Loss: 5.185584541322896e-06, Test Loss: 5.185584541322896e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 607, Train Loss: 5.185584541322896e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 608, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 609, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 610, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 611, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 612, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 613, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 614, Train Loss: 5.155782218935201e-06, Test Loss: 5.155782218935201e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 615, Train Loss: 5.155782218935201e-06, Test Loss: 5.096178483654512e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 616, Train Loss: 5.096178483654512e-06, Test Loss: 5.096178483654512e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 617, Train Loss: 5.096178483654512e-06, Test Loss: 5.096178483654512e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 618, Train Loss: 5.096178028907161e-06, Test Loss: 5.096178028907161e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 619, Train Loss: 5.096178028907161e-06, Test Loss: 5.096178028907161e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 620, Train Loss: 5.096178483654512e-06, Test Loss: 5.096178483654512e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 621, Train Loss: 5.096178483654512e-06, Test Loss: 5.096178483654512e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 622, Train Loss: 5.096178028907161e-06, Test Loss: 5.0365738388791215e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 623, Train Loss: 5.0365738388791215e-06, Test Loss: 5.036574293626472e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 624, Train Loss: 5.036574293626472e-06, Test Loss: 5.036574293626472e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 625, Train Loss: 5.036574293626472e-06, Test Loss: 5.006771971238777e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 626, Train Loss: 5.006771971238777e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 627, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 628, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 629, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 630, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 631, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 632, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 633, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 634, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 635, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 636, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 637, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 638, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 639, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 640, Train Loss: 4.976969648851082e-06, Test Loss: 4.976969648851082e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 641, Train Loss: 4.976969648851082e-06, Test Loss: 4.947167781210737e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 642, Train Loss: 4.947167781210737e-06, Test Loss: 4.8875635911826976e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 643, Train Loss: 4.8875635911826976e-06, Test Loss: 4.8875635911826976e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 644, Train Loss: 4.8875635911826976e-06, Test Loss: 4.857761268795002e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 645, Train Loss: 4.857761268795002e-06, Test Loss: 4.857761268795002e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 646, Train Loss: 4.857761268795002e-06, Test Loss: 4.857761268795002e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 647, Train Loss: 4.857761268795002e-06, Test Loss: 4.857761268795002e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 648, Train Loss: 4.857761268795002e-06, Test Loss: 4.857761268795002e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 649, Train Loss: 4.857761268795002e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 650, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 651, Train Loss: 4.827958946407307e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 652, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 653, Train Loss: 4.827958946407307e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 654, Train Loss: 4.827958946407307e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 655, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 656, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 657, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 658, Train Loss: 4.827959401154658e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 659, Train Loss: 4.827959401154658e-06, Test Loss: 4.827958946407307e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 660, Train Loss: 4.827958946407307e-06, Test Loss: 4.827959401154658e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 661, Train Loss: 4.827958946407307e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 662, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 663, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 664, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 665, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 666, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 667, Train Loss: 4.798157533514313e-06, Test Loss: 4.798157533514313e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 668, Train Loss: 4.798157533514313e-06, Test Loss: 4.768355211126618e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 669, Train Loss: 4.768355211126618e-06, Test Loss: 4.768355211126618e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 670, Train Loss: 4.768355211126618e-06, Test Loss: 4.738553343486274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 671, Train Loss: 4.738552888738923e-06, Test Loss: 4.738552888738923e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 672, Train Loss: 4.738553343486274e-06, Test Loss: 4.738553343486274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 673, Train Loss: 4.738552888738923e-06, Test Loss: 4.738553343486274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 674, Train Loss: 4.738552888738923e-06, Test Loss: 4.738552888738923e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 675, Train Loss: 4.738552888738923e-06, Test Loss: 4.738553343486274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 676, Train Loss: 4.738553343486274e-06, Test Loss: 4.738553343486274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 677, Train Loss: 4.738552888738923e-06, Test Loss: 4.708751021098578e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 678, Train Loss: 4.708751021098578e-06, Test Loss: 4.678948698710883e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 679, Train Loss: 4.678948698710883e-06, Test Loss: 4.678948698710883e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 680, Train Loss: 4.678948698710883e-06, Test Loss: 4.678948698710883e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 681, Train Loss: 4.678948698710883e-06, Test Loss: 4.678948698710883e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 682, Train Loss: 4.678948698710883e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 683, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 684, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 685, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 686, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 687, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 688, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 689, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 690, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 691, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 692, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 693, Train Loss: 4.649146831070539e-06, Test Loss: 4.649146831070539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 694, Train Loss: 4.649146831070539e-06, Test Loss: 4.619344508682843e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 695, Train Loss: 4.619344508682843e-06, Test Loss: 4.589542186295148e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 696, Train Loss: 4.589542186295148e-06, Test Loss: 4.589542186295148e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 697, Train Loss: 4.589542186295148e-06, Test Loss: 4.589542186295148e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 698, Train Loss: 4.589542186295148e-06, Test Loss: 4.589542186295148e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 699, Train Loss: 4.589542186295148e-06, Test Loss: 4.5597403186548036e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 700, Train Loss: 4.5597403186548036e-06, Test Loss: 4.5597403186548036e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 701, Train Loss: 4.5597403186548036e-06, Test Loss: 4.5597403186548036e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 702, Train Loss: 4.5597403186548036e-06, Test Loss: 4.559739863907453e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 703, Train Loss: 4.559739863907453e-06, Test Loss: 4.5597403186548036e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 704, Train Loss: 4.559739863907453e-06, Test Loss: 4.529937996267108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 705, Train Loss: 4.529937996267108e-06, Test Loss: 4.529937996267108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 706, Train Loss: 4.529937996267108e-06, Test Loss: 4.529937996267108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 707, Train Loss: 4.529937996267108e-06, Test Loss: 4.529937996267108e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 708, Train Loss: 4.529937996267108e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 709, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 710, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 711, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 712, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 713, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 714, Train Loss: 4.500136128626764e-06, Test Loss: 4.500136128626764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 715, Train Loss: 4.500136128626764e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 716, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 717, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 718, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 719, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 720, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 721, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 722, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 723, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 724, Train Loss: 4.4703338062390685e-06, Test Loss: 4.4703338062390685e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 725, Train Loss: 4.4703338062390685e-06, Test Loss: 4.440531483851373e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 726, Train Loss: 4.440531483851373e-06, Test Loss: 4.410729161463678e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 727, Train Loss: 4.410729616211029e-06, Test Loss: 4.410729161463678e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 728, Train Loss: 4.410729616211029e-06, Test Loss: 4.410729161463678e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 729, Train Loss: 4.410729161463678e-06, Test Loss: 4.410729161463678e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 730, Train Loss: 4.410729616211029e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 731, Train Loss: 4.3809272938233335e-06, Test Loss: 4.380927748570684e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 732, Train Loss: 4.3809272938233335e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 733, Train Loss: 4.3809272938233335e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 734, Train Loss: 4.3809272938233335e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 735, Train Loss: 4.380927748570684e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 736, Train Loss: 4.3809272938233335e-06, Test Loss: 4.380927748570684e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 737, Train Loss: 4.3809272938233335e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 738, Train Loss: 4.3809272938233335e-06, Test Loss: 4.3809272938233335e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 739, Train Loss: 4.3809272938233335e-06, Test Loss: 4.380927748570684e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 740, Train Loss: 4.380927748570684e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 741, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 742, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 743, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 744, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 745, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 746, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 747, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 748, Train Loss: 4.351125426182989e-06, Test Loss: 4.351125426182989e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 749, Train Loss: 4.351125426182989e-06, Test Loss: 4.321323103795294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 750, Train Loss: 4.321323103795294e-06, Test Loss: 4.291520781407598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 751, Train Loss: 4.291520781407598e-06, Test Loss: 4.291520781407598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 752, Train Loss: 4.291521236154949e-06, Test Loss: 4.291520781407598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 753, Train Loss: 4.291520781407598e-06, Test Loss: 4.291521236154949e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 754, Train Loss: 4.291520781407598e-06, Test Loss: 4.291520781407598e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 755, Train Loss: 4.291521236154949e-06, Test Loss: 4.261718913767254e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 756, Train Loss: 4.261718459019903e-06, Test Loss: 4.261718459019903e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 757, Train Loss: 4.261718913767254e-06, Test Loss: 4.261718459019903e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 758, Train Loss: 4.261718459019903e-06, Test Loss: 4.231916591379559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 759, Train Loss: 4.231916591379559e-06, Test Loss: 4.231916591379559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 760, Train Loss: 4.231916591379559e-06, Test Loss: 4.231916591379559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 761, Train Loss: 4.231916591379559e-06, Test Loss: 4.231916591379559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 762, Train Loss: 4.231916591379559e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 763, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 764, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 765, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 766, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 767, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 768, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 769, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 770, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 771, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 772, Train Loss: 4.202114723739214e-06, Test Loss: 4.202114723739214e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 773, Train Loss: 4.202114723739214e-06, Test Loss: 4.172312401351519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 774, Train Loss: 4.172312401351519e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 775, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 776, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 777, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 778, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 779, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 780, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 781, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 782, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 783, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 784, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 785, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 786, Train Loss: 4.142510078963824e-06, Test Loss: 4.142510078963824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 787, Train Loss: 4.142510078963824e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 788, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 789, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 790, Train Loss: 4.112708211323479e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 791, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 792, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 793, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 794, Train Loss: 4.112707756576128e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 795, Train Loss: 4.112708211323479e-06, Test Loss: 4.112707756576128e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 796, Train Loss: 4.112708211323479e-06, Test Loss: 4.112708211323479e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 797, Train Loss: 4.112707756576128e-06, Test Loss: 4.082905888935784e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 798, Train Loss: 4.082905888935784e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 799, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 800, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 801, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 802, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 803, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 804, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 805, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 806, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 807, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 808, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 809, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 810, Train Loss: 4.0531040212954395e-06, Test Loss: 4.0531040212954395e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 811, Train Loss: 4.0531040212954395e-06, Test Loss: 4.023301698907744e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 812, Train Loss: 4.023301698907744e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 813, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 814, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 815, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 816, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 817, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 818, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 819, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 820, Train Loss: 3.993499376520049e-06, Test Loss: 3.993499376520049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 821, Train Loss: 3.993499376520049e-06, Test Loss: 3.933895186492009e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 822, Train Loss: 3.933895186492009e-06, Test Loss: 3.933895186492009e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 823, Train Loss: 3.933895186492009e-06, Test Loss: 3.904092864104314e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 824, Train Loss: 3.904093318851665e-06, Test Loss: 3.904093318851665e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 825, Train Loss: 3.904093318851665e-06, Test Loss: 3.904093318851665e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 826, Train Loss: 3.904092864104314e-06, Test Loss: 3.904093318851665e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 827, Train Loss: 3.904092864104314e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 828, Train Loss: 3.874290996463969e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 829, Train Loss: 3.874290996463969e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 830, Train Loss: 3.874290996463969e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 831, Train Loss: 3.874290996463969e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 832, Train Loss: 3.874290996463969e-06, Test Loss: 3.874290996463969e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 833, Train Loss: 3.874290996463969e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 834, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 835, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 836, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 837, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 838, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 839, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 840, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 841, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 842, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 843, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 844, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 845, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 846, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 847, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 848, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 849, Train Loss: 3.844488674076274e-06, Test Loss: 3.844488674076274e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 850, Train Loss: 3.844488674076274e-06, Test Loss: 3.7848847114219097e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 851, Train Loss: 3.7848844840482343e-06, Test Loss: 3.7848844840482343e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 852, Train Loss: 3.7848844840482343e-06, Test Loss: 3.7848844840482343e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 853, Train Loss: 3.7848844840482343e-06, Test Loss: 3.755082161660539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 854, Train Loss: 3.7550823890342144e-06, Test Loss: 3.755082161660539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 855, Train Loss: 3.7550823890342144e-06, Test Loss: 3.755082161660539e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 856, Train Loss: 3.7550823890342144e-06, Test Loss: 3.7252802940201946e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 857, Train Loss: 3.7252802940201946e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 858, Train Loss: 3.725280066646519e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 859, Train Loss: 3.7252802940201946e-06, Test Loss: 3.7252802940201946e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 860, Train Loss: 3.725280066646519e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 861, Train Loss: 3.7252802940201946e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 862, Train Loss: 3.7252802940201946e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 863, Train Loss: 3.725280066646519e-06, Test Loss: 3.7252802940201946e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 864, Train Loss: 3.7252798392728437e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 865, Train Loss: 3.7252798392728437e-06, Test Loss: 3.7252802940201946e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 866, Train Loss: 3.725280066646519e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 867, Train Loss: 3.7252798392728437e-06, Test Loss: 3.7252802940201946e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 868, Train Loss: 3.7252802940201946e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 869, Train Loss: 3.7252798392728437e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 870, Train Loss: 3.7252802940201946e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 871, Train Loss: 3.7252798392728437e-06, Test Loss: 3.725280066646519e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 872, Train Loss: 3.725280066646519e-06, Test Loss: 3.7252798392728437e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 873, Train Loss: 3.7252798392728437e-06, Test Loss: 3.6954779716324992e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 874, Train Loss: 3.6954779716324992e-06, Test Loss: 3.6954779716324992e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 875, Train Loss: 3.6954779716324992e-06, Test Loss: 3.6954779716324992e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 876, Train Loss: 3.6954779716324992e-06, Test Loss: 3.6954779716324992e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 877, Train Loss: 3.6954779716324992e-06, Test Loss: 3.6954779716324992e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 878, Train Loss: 3.6954779716324992e-06, Test Loss: 3.665676103992155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 879, Train Loss: 3.6656758766184794e-06, Test Loss: 3.665675649244804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 880, Train Loss: 3.6656758766184794e-06, Test Loss: 3.665676103992155e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 881, Train Loss: 3.665676103992155e-06, Test Loss: 3.665675649244804e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 882, Train Loss: 3.6656758766184794e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 883, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 884, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 885, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 886, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 887, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 888, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 889, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 890, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 891, Train Loss: 3.6358737816044595e-06, Test Loss: 3.6358737816044595e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 892, Train Loss: 3.6358737816044595e-06, Test Loss: 3.606071459216764e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 893, Train Loss: 3.606071459216764e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 894, Train Loss: 3.5762695915764198e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 895, Train Loss: 3.5762693642027443e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 896, Train Loss: 3.5762693642027443e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 897, Train Loss: 3.5762693642027443e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 898, Train Loss: 3.576269136829069e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 899, Train Loss: 3.576269136829069e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 900, Train Loss: 3.5762695915764198e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 901, Train Loss: 3.5762695915764198e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 902, Train Loss: 3.5762693642027443e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 903, Train Loss: 3.5762693642027443e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 904, Train Loss: 3.576269136829069e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 905, Train Loss: 3.5762695915764198e-06, Test Loss: 3.5762693642027443e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 906, Train Loss: 3.576269136829069e-06, Test Loss: 3.5762695915764198e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 907, Train Loss: 3.576269136829069e-06, Test Loss: 3.546467041815049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 908, Train Loss: 3.546467041815049e-06, Test Loss: 3.546467041815049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 909, Train Loss: 3.546467041815049e-06, Test Loss: 3.546467041815049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 910, Train Loss: 3.546467041815049e-06, Test Loss: 3.5464672691887245e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 911, Train Loss: 3.5464672691887245e-06, Test Loss: 3.5464672691887245e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 912, Train Loss: 3.546467041815049e-06, Test Loss: 3.546467041815049e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 913, Train Loss: 3.546467041815049e-06, Test Loss: 3.5464672691887245e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 914, Train Loss: 3.5464672691887245e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 915, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 916, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 917, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 918, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 919, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 920, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 921, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 922, Train Loss: 3.5166651741747046e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 923, Train Loss: 3.5166651741747046e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 924, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 925, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 926, Train Loss: 3.5166651741747046e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 927, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 928, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 929, Train Loss: 3.516664946801029e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 930, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 931, Train Loss: 3.516664946801029e-06, Test Loss: 3.5166651741747046e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 932, Train Loss: 3.5166651741747046e-06, Test Loss: 3.516664946801029e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 933, Train Loss: 3.516664946801029e-06, Test Loss: 3.4868628517870093e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 934, Train Loss: 3.4868628517870093e-06, Test Loss: 3.4868630791606847e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 935, Train Loss: 3.4868630791606847e-06, Test Loss: 3.4868628517870093e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 936, Train Loss: 3.4868630791606847e-06, Test Loss: 3.4868630791606847e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 937, Train Loss: 3.4868630791606847e-06, Test Loss: 3.4868628517870093e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 938, Train Loss: 3.4868628517870093e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 939, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 940, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 941, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 942, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 943, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 944, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 945, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 946, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 947, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 948, Train Loss: 3.427258434385294e-06, Test Loss: 3.427258434385294e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 949, Train Loss: 3.427258434385294e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 950, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 951, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 952, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 953, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 954, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 955, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 956, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 957, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 958, Train Loss: 3.3974563393712742e-06, Test Loss: 3.3974563393712742e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 959, Train Loss: 3.3974563393712742e-06, Test Loss: 3.367654016983579e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 960, Train Loss: 3.3676542443572544e-06, Test Loss: 3.3676542443572544e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 961, Train Loss: 3.367654016983579e-06, Test Loss: 3.337851921969559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 962, Train Loss: 3.337851921969559e-06, Test Loss: 3.337851921969559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 963, Train Loss: 3.337851921969559e-06, Test Loss: 3.337851921969559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 964, Train Loss: 3.337851921969559e-06, Test Loss: 3.337851921969559e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 965, Train Loss: 3.337851921969559e-06, Test Loss: 3.3080495995818637e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 966, Train Loss: 3.3080495995818637e-06, Test Loss: 3.3080495995818637e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 967, Train Loss: 3.3080495995818637e-06, Test Loss: 3.3080495995818637e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 968, Train Loss: 3.3080495995818637e-06, Test Loss: 3.3080495995818637e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 969, Train Loss: 3.3080495995818637e-06, Test Loss: 3.3080495995818637e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 970, Train Loss: 3.3080495995818637e-06, Test Loss: 3.278247504567844e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 971, Train Loss: 3.278247504567844e-06, Test Loss: 3.278247504567844e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 972, Train Loss: 3.2782472771941684e-06, Test Loss: 3.278247504567844e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 973, Train Loss: 3.2782472771941684e-06, Test Loss: 3.2782472771941684e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 974, Train Loss: 3.278247504567844e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 975, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 976, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 977, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 978, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 979, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 980, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 981, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 982, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 983, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 984, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 985, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 986, Train Loss: 3.248445409553824e-06, Test Loss: 3.248445409553824e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 987, Train Loss: 3.248445409553824e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 988, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 989, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 990, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 991, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 992, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 993, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 994, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 995, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 996, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 997, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 998, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 999, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n",
      "Epoch 1000, Train Loss: 3.2186430871661287e-06, Test Loss: 3.2186430871661287e-06, Train Error Rate: 0.0, Test Error Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    predicted_classes = torch.argmax(y_pred, dim=1)\n",
    "    correct_predictions = torch.eq(predicted_classes, y_true).sum().item()\n",
    "    accuracy = correct_predictions / y_true.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_error_rates = []\n",
    "test_error_rates = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += calculate_accuracy(outputs, targets)\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_dataloader))\n",
    "    train_error_rates.append(1 - (train_accuracy / len(train_dataloader)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += calculate_accuracy(outputs, targets)\n",
    "    \n",
    "    test_losses.append(test_loss / len(train_dataloader))\n",
    "    test_error_rates.append(1 - (test_accuracy / len(train_dataloader)))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}, Train Error Rate: {train_error_rates[-1]}, Test Error Rate: {test_error_rates[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_error_rates, label='Train Error Rate')\n",
    "plt.plot(test_error_rates, label='Test Error Rate')\n",
    "plt.title('Error Rate over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities (correct label is index 0): 0.8462716937065125\n",
      "Probabilities (correct label is index 0): 0.8229128122329712\n",
      "Probabilities (correct label is index 0): 0.13534393906593323\n",
      "Probabilities (correct label is index 0): 0.014202769845724106\n",
      "Probabilities (correct label is index 0): 0.07615182548761368\n",
      "Probabilities (correct label is index 0): 0.013588770292699337\n",
      "Probabilities (correct label is index 0): 0.01116337813436985\n",
      "Probabilities (correct label is index 0): 0.015368434600532055\n",
      "Probabilities (correct label is index 0): 0.023910868912935257\n",
      "Probabilities (correct label is index 0): 0.26403290033340454\n",
      "Probabilities (correct label is index 0): 0.8921283483505249\n",
      "Probabilities (correct label is index 0): 0.005871458910405636\n",
      "Probabilities (correct label is index 0): 0.04214233160018921\n",
      "Probabilities (correct label is index 0): 0.25380846858024597\n",
      "Probabilities (correct label is index 0): 0.911517322063446\n"
     ]
    }
   ],
   "source": [
    "orbit1 = half_sphere_output[:60]\n",
    "orbit2 = half_sphere_output[60:120]\n",
    "orbit3 = half_sphere_output[120:]\n",
    "\n",
    "shift_by = [3,5,6,10,13,17,21,22,23,29,30,41,47,55,58]\n",
    "for shift in shift_by:\n",
    "    test_accuracy = 0.0\n",
    "    orbit1_rolled = np.roll(orbit1, shift)\n",
    "    orbit2_rolled = np.roll(orbit2, shift)\n",
    "    orbit3_rolled = np.roll(orbit3, shift)\n",
    "    half_sphere_rolled = torch.tensor(list(orbit1_rolled) + list(orbit2_rolled) + list(orbit3_rolled), dtype = torch.float32)\n",
    "    #print(half_sphere_rolled.shape)\n",
    "    outputs = model(half_sphere_rolled)\n",
    "    #print(outputs)\n",
    "    print(f'Probabilities (correct label is index 0): {torch.softmax(outputs, dim=0)[0]}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m4r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
